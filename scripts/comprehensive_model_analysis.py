#!/usr/bin/env python
# coding: utf-8

"""
åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯
- æ™‚é–“è»¸æ•´åˆæ€§ç¢ºèª
- ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©³ç´°åˆ†æ
- æ”¹å–„æ–¹é‡ã®ææ¡ˆ
"""

import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import sys
import os
from datetime import datetime, timedelta

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

warnings.filterwarnings('ignore')

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import SMOOTHING_WINDOWS, FEATURE_SELECTION_THRESHOLD, TEST_SIZE

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.preprocessing import (
    filter_temperature_outliers,
    create_temperature_difference_targets,
    prepare_time_features,
    get_time_based_train_test_split,
    filter_high_value_targets
)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.feature_engineering import create_difference_prediction_pipeline

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.training import train_temperature_difference_model

# è©•ä¾¡é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.evaluation import evaluate_temperature_difference_model


def check_data_leakage(df, zone, horizon, time_diff_seconds):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯

    Parameters:
    -----------
    df : DataFrame
        å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
    time_diff_seconds : float
        ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”

    Returns:
    --------
    dict
        ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯çµæœ
    """
    print("\\nğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯é–‹å§‹...")

    results = {
        'potential_leaks': [],
        'future_features': [],
        'temporal_consistency': {},
        'feature_timing': {}
    }

    # 1. æœªæ¥æƒ…å ±ã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹ç‰¹å¾´é‡ã®æ¤œå‡º
    future_keywords = ['future', 'lag', 'shift', 'lead']

    for col in df.columns:
        col_lower = col.lower()
        for keyword in future_keywords:
            if keyword in col_lower:
                if 'future' in col_lower:
                    results['future_features'].append(col)
                else:
                    results['potential_leaks'].append(col)

    # 2. ç›®çš„å¤‰æ•°ã¨ã®æ™‚é–“çš„é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
    target_col = f'temp_diff_{zone}_future_{horizon}'
    if target_col in df.columns:
        shift_steps = int(horizon * 60 / time_diff_seconds)

        # åŸºæœ¬æ¸©åº¦ã‚«ãƒ©ãƒ ã¨ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
        base_temp_col = f'sens_temp_{zone}'
        if base_temp_col in df.columns:
            # ç¾åœ¨æ™‚ç‚¹ã®æ¸©åº¦ã¨æœªæ¥ã®å·®åˆ†ã®ç›¸é–¢
            correlation = df[base_temp_col].corr(df[target_col])
            results['temporal_consistency']['current_temp_vs_future_diff'] = correlation

            # æœªæ¥ã®æ¸©åº¦ã‚’é€†ç®—ã—ã¦ãƒã‚§ãƒƒã‚¯
            df_temp = df.copy()
            df_temp['future_temp_calculated'] = df_temp[base_temp_col] + df_temp[target_col]
            df_temp['future_temp_actual'] = df_temp[base_temp_col].shift(-shift_steps)

            # è¨ˆç®—ã•ã‚ŒãŸæœªæ¥æ¸©åº¦ã¨å®Ÿéš›ã®æœªæ¥æ¸©åº¦ã®ä¸€è‡´åº¦
            valid_mask = df_temp[['future_temp_calculated', 'future_temp_actual']].notna().all(axis=1)
            if valid_mask.sum() > 0:
                calc_vs_actual_corr = df_temp.loc[valid_mask, 'future_temp_calculated'].corr(
                    df_temp.loc[valid_mask, 'future_temp_actual']
                )
                results['temporal_consistency']['calculated_vs_actual_future'] = calc_vs_actual_corr

                # RMSEè¨ˆç®—
                rmse = np.sqrt(np.mean((
                    df_temp.loc[valid_mask, 'future_temp_calculated'] -
                    df_temp.loc[valid_mask, 'future_temp_actual']
                )**2))
                results['temporal_consistency']['future_temp_rmse'] = rmse

    # 3. ç‰¹å¾´é‡ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    horizon_minutes = horizon
    expected_shift = pd.Timedelta(minutes=horizon_minutes)

    for col in results['future_features']:
        if f'future_{horizon}' in col:
            base_col = col.replace(f'_future_{horizon}', '')
            if base_col in df.columns:
                # éãƒŠãƒ³å€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å·®åˆ†ã‚’ãƒã‚§ãƒƒã‚¯
                future_valid = df[col].notna()
                base_valid = df[base_col].notna()

                if future_valid.sum() > 0 and base_valid.sum() > 0:
                    # æœ€åˆã¨æœ€å¾Œã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å·®
                    future_first = df.index[future_valid][0] if future_valid.sum() > 0 else None
                    future_last = df.index[future_valid][-1] if future_valid.sum() > 0 else None
                    base_first = df.index[base_valid][0] if base_valid.sum() > 0 else None
                    base_last = df.index[base_valid][-1] if base_valid.sum() > 0 else None

                    results['feature_timing'][col] = {
                        'future_first': future_first,
                        'future_last': future_last,
                        'base_first': base_first,
                        'base_last': base_last,
                        'expected_shift_minutes': horizon_minutes
                    }

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯å®Œäº†")
    print(f"   - æœªæ¥ç‰¹å¾´é‡æ•°: {len(results['future_features'])}")
    print(f"   - æ½œåœ¨çš„ãƒªã‚¹ã‚¯ç‰¹å¾´é‡æ•°: {len(results['potential_leaks'])}")

    return results


def analyze_model_performance(model, X_test, y_test, feature_names, current_temps=None):
    """
    ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©³ç´°åˆ†æ

    Parameters:
    -----------
    model : trained model
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    X_test : DataFrame
        ãƒ†ã‚¹ãƒˆç‰¹å¾´é‡
    y_test : Series
        ãƒ†ã‚¹ãƒˆç›®çš„å¤‰æ•°
    feature_names : list
        ç‰¹å¾´é‡å
    current_temps : Series, optional
        ç¾åœ¨æ¸©åº¦ãƒ‡ãƒ¼ã‚¿

    Returns:
    --------
    dict
        è©³ç´°åˆ†æçµæœ
    """
    print("\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©³ç´°åˆ†æé–‹å§‹...")

    # äºˆæ¸¬å®Ÿè¡Œ
    y_pred = model.predict(X_test)

    # åŸºæœ¬æ€§èƒ½æŒ‡æ¨™
    if current_temps is not None:
        metrics = evaluate_temperature_difference_model(y_test, y_pred, current_temps)
    else:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        metrics = {
            'diff_rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'diff_mae': mean_absolute_error(y_test, y_pred),
            'diff_r2': r2_score(y_test, y_pred)
        }

    # è©³ç´°åˆ†æ
    analysis = {
        'basic_metrics': metrics,
        'feature_importance': {},
        'residual_analysis': {},
        'prediction_distribution': {},
        'temporal_patterns': {},
        'outlier_analysis': {}
    }

    # 1. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        analysis['feature_importance'] = {
            'top_10': feature_importance_df.head(10).to_dict('records'),
            'total_features': len(feature_names),
            'importance_concentration': importances.max() / importances.sum(),
            'effective_features': (importances > 0.01).sum()
        }

    # 2. æ®‹å·®åˆ†æ
    residuals = y_test - y_pred
    analysis['residual_analysis'] = {
        'residual_mean': float(residuals.mean()),
        'residual_std': float(residuals.std()),
        'residual_skewness': float(residuals.skew()),
        'residual_kurtosis': float(residuals.kurtosis()),
        'residual_autocorr': float(residuals.autocorr()) if len(residuals) > 1 else 0.0
    }

    # 3. äºˆæ¸¬åˆ†å¸ƒåˆ†æ
    analysis['prediction_distribution'] = {
        'pred_mean': float(y_pred.mean()),
        'pred_std': float(y_pred.std()),
        'actual_mean': float(y_test.mean()),
        'actual_std': float(y_test.std()),
        'pred_range': [float(y_pred.min()), float(y_pred.max())],
        'actual_range': [float(y_test.min()), float(y_test.max())]
    }

    # 4. æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    if hasattr(y_test, 'index'):
        test_df = pd.DataFrame({
            'actual': y_test,
            'predicted': y_pred,
            'residual': residuals
        }, index=y_test.index)

        # æ™‚é–“åˆ¥æ€§èƒ½
        test_df['hour'] = test_df.index.hour
        hourly_performance = test_df.groupby('hour')['residual'].agg(['mean', 'std']).abs()

        analysis['temporal_patterns'] = {
            'best_hour': int(hourly_performance['mean'].idxmin()),
            'worst_hour': int(hourly_performance['mean'].idxmax()),
            'hourly_variation': float(hourly_performance['mean'].std()),
            'time_dependency': float(test_df['residual'].autocorr())
        }

    # 5. å¤–ã‚Œå€¤åˆ†æ
    residual_threshold = 2 * residuals.std()
    outliers = residuals.abs() > residual_threshold

    analysis['outlier_analysis'] = {
        'outlier_count': int(outliers.sum()),
        'outlier_rate': float(outliers.mean()),
        'max_error': float(residuals.abs().max()),
        'outlier_threshold': float(residual_threshold)
    }

    print(f"âœ… ãƒ¢ãƒ‡ãƒ«æ€§èƒ½åˆ†æå®Œäº†")
    print(f"   - RMSE: {metrics.get('diff_rmse', metrics.get('restoration_rmse', 'N/A')):.4f}")
    print(f"   - å¤–ã‚Œå€¤ç‡: {analysis['outlier_analysis']['outlier_rate']:.1%}")
    print(f"   - æœ‰åŠ¹ç‰¹å¾´é‡æ•°: {analysis['feature_importance'].get('effective_features', 'N/A')}")

    return analysis


def generate_improvement_recommendations(leak_results, performance_analysis):
    """
    æ”¹å–„ææ¡ˆã®ç”Ÿæˆ

    Parameters:
    -----------
    leak_results : dict
        ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯çµæœ
    performance_analysis : dict
        æ€§èƒ½åˆ†æçµæœ

    Returns:
    --------
    dict
        æ”¹å–„ææ¡ˆ
    """
    print("\\nğŸ’¡ æ”¹å–„ææ¡ˆç”Ÿæˆä¸­...")

    recommendations = {
        'data_quality': [],
        'feature_engineering': [],
        'model_improvements': [],
        'validation_improvements': [],
        'priority_actions': []
    }

    # ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„
    if leak_results['potential_leaks']:
        recommendations['data_quality'].append({
            'issue': 'Potential data leakage detected',
            'action': f"Review {len(leak_results['potential_leaks'])} features with leak risk",
            'priority': 'HIGH',
            'features': leak_results['potential_leaks'][:5]  # æœ€åˆã®5å€‹
        })

    # æ™‚é–“æ•´åˆæ€§ã®å•é¡Œ
    future_temp_corr = leak_results['temporal_consistency'].get('calculated_vs_actual_future')
    if future_temp_corr and future_temp_corr < 0.95:
        recommendations['data_quality'].append({
            'issue': 'Temporal inconsistency detected',
            'action': f"Future temperature correlation is {future_temp_corr:.3f}, should be >0.95",
            'priority': 'HIGH'
        })

    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ”¹å–„
    importance_analysis = performance_analysis['feature_importance']
    if importance_analysis.get('importance_concentration', 0) > 0.3:
        recommendations['feature_engineering'].append({
            'issue': 'High feature importance concentration',
            'action': 'Add more diverse features to reduce dependency on single feature',
            'priority': 'MEDIUM'
        })

    effective_features = importance_analysis.get('effective_features', 0)
    total_features = importance_analysis.get('total_features', 0)
    if total_features > 0 and effective_features / total_features < 0.3:
        recommendations['feature_engineering'].append({
            'issue': 'Low feature utilization',
            'action': f"Only {effective_features}/{total_features} features are effective",
            'priority': 'MEDIUM'
        })

    # ãƒ¢ãƒ‡ãƒ«æ”¹å–„
    residual_analysis = performance_analysis['residual_analysis']
    if abs(residual_analysis['residual_skewness']) > 1.0:
        recommendations['model_improvements'].append({
            'issue': 'Skewed residuals',
            'action': 'Consider robust loss functions or data transformation',
            'priority': 'MEDIUM'
        })

    if residual_analysis['residual_autocorr'] > 0.1:
        recommendations['model_improvements'].append({
            'issue': 'Temporal correlation in residuals',
            'action': 'Add temporal features or consider sequence models',
            'priority': 'HIGH'
        })

    # å¤–ã‚Œå€¤å¯¾ç­–
    outlier_rate = performance_analysis['outlier_analysis']['outlier_rate']
    if outlier_rate > 0.05:  # 5%ä»¥ä¸Š
        recommendations['model_improvements'].append({
            'issue': f'High outlier rate: {outlier_rate:.1%}',
            'action': 'Implement robust training or better outlier detection',
            'priority': 'MEDIUM'
        })

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ”¹å–„
    temporal_patterns = performance_analysis.get('temporal_patterns', {})
    if temporal_patterns.get('hourly_variation', 0) > 0.1:
        recommendations['validation_improvements'].append({
            'issue': 'High hourly performance variation',
            'action': 'Implement time-stratified validation',
            'priority': 'MEDIUM'
        })

    # å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    high_priority = [r for cat in recommendations.values()
                    for r in (cat if isinstance(cat, list) else [cat])
                    if isinstance(r, dict) and r.get('priority') == 'HIGH']

    recommendations['priority_actions'] = high_priority[:3]  # æœ€å„ªå…ˆ3ã¤

    print(f"âœ… æ”¹å–„ææ¡ˆç”Ÿæˆå®Œäº†")
    print(f"   - é«˜å„ªå…ˆåº¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {len(high_priority)}")
    print(f"   - ç·ææ¡ˆæ•°: {sum(len(v) for v in recommendations.values() if isinstance(v, list))}")

    return recommendations


def create_comprehensive_analysis_report(zone=1, horizon=15):
    """
    åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ

    Parameters:
    -----------
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³

    Returns:
    --------
    dict
        åŒ…æ‹¬çš„åˆ†æçµæœ
    """
    print("ğŸ”¥ åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«åˆ†æé–‹å§‹")
    print(f"å¯¾è±¡: ã‚¾ãƒ¼ãƒ³{zone}, äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³{horizon}åˆ†")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_path = project_root / "AllDayData.csv"
    print(f"\\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")

    df = pd.read_csv(data_path, low_memory=False)

    # æ™‚é–“åˆ—ã®ç¢ºèªã¨è¨­å®š
    if 'time_stamp' in df.columns:
        df['datetime'] = pd.to_datetime(df['time_stamp'])
        df = df.set_index('datetime')
    elif 'datetime' in df.columns:
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.set_index('datetime')
    else:
        raise ValueError("æ™‚é–“åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'time_stamp'ã¾ãŸã¯'datetime'åˆ—ãŒå¿…è¦ã§ã™ã€‚")

    time_diff_seconds = df.index.to_series().diff().dropna().value_counts().index[0].total_seconds()

    # åŸºæœ¬å‰å‡¦ç†
    df = filter_temperature_outliers(df)
    df = prepare_time_features(df)

    # å·®åˆ†äºˆæ¸¬ç”¨ç›®çš„å¤‰æ•°ã®ä½œæˆ
    df_with_diff_targets = create_temperature_difference_targets(
        df, [zone], [horizon], pd.Timedelta(seconds=time_diff_seconds)
    )

    # 1. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯
    leak_results = check_data_leakage(df_with_diff_targets, zone, horizon, time_diff_seconds)

    # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    df_processed, selected_features, feature_info = create_difference_prediction_pipeline(
        df=df_with_diff_targets,
        zone_nums=[zone],
        horizons_minutes=[horizon],
        time_diff_seconds=time_diff_seconds,
        smoothing_window=SMOOTHING_WINDOWS[0] if SMOOTHING_WINDOWS else 5,
        feature_selection_threshold=FEATURE_SELECTION_THRESHOLD
    )

    # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    diff_target_col = f'temp_diff_{zone}_future_{horizon}'
    feature_cols = [col for col in selected_features if col in df_processed.columns]
    valid_data = df_processed.dropna(subset=[diff_target_col] + feature_cols)

    # é«˜å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ25%ileï¼‰
    abs_diff_col = f'abs_temp_diff_{zone}_future_{horizon}'
    valid_data[abs_diff_col] = valid_data[diff_target_col].abs()
    filtered_data, filter_info = filter_high_value_targets(
        valid_data, [abs_diff_col], percentile=25
    )

    # æ™‚ç³»åˆ—åˆ†å‰²
    train_df, test_df = get_time_based_train_test_split(filtered_data, test_size=TEST_SIZE)

    X_train = train_df[feature_cols]
    y_train_diff = train_df[diff_target_col]
    X_test = test_df[feature_cols]
    y_test_diff = test_df[diff_target_col]
    current_temps_test = test_df[f'sens_temp_{zone}']

    # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = train_temperature_difference_model(X_train, y_train_diff)

    # 5. æ€§èƒ½åˆ†æ
    performance_analysis = analyze_model_performance(
        model, X_test, y_test_diff, feature_cols, current_temps_test
    )

    # 6. æ”¹å–„ææ¡ˆç”Ÿæˆ
    recommendations = generate_improvement_recommendations(leak_results, performance_analysis)

    # çµæœçµ±åˆ
    comprehensive_results = {
        'metadata': {
            'zone': zone,
            'horizon': horizon,
            'analysis_timestamp': datetime.now().isoformat(),
            'data_shape': {
                'original': df.shape,
                'processed': df_processed.shape,
                'filtered': filtered_data.shape,
                'train': X_train.shape,
                'test': X_test.shape
            }
        },
        'data_leak_analysis': leak_results,
        'performance_analysis': performance_analysis,
        'improvement_recommendations': recommendations,
        'filter_info': filter_info
    }

    # çµæœä¿å­˜
    output_dir = project_root / "Output"
    output_dir.mkdir(exist_ok=True)

    results_file = output_dir / f"comprehensive_analysis_zone{zone}_horizon{horizon}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)

    print(f"\\nçµæœã‚’ä¿å­˜: {results_file}")

    return comprehensive_results


def print_analysis_summary(results):
    """
    åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    """
    print("\\n" + "="*80)
    print("ğŸ¯ åŒ…æ‹¬çš„åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print("="*80)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = results['metadata']
    print(f"ğŸ“Š åŸºæœ¬æƒ…å ±:")
    print(f"   ã‚¾ãƒ¼ãƒ³: {metadata['zone']}, ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³: {metadata['horizon']}åˆ†")
    print(f"   ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {metadata['data_shape']['original']} â†’ {metadata['data_shape']['filtered']}")

    # æ€§èƒ½æŒ‡æ¨™
    performance = results['performance_analysis']['basic_metrics']
    print(f"\\nğŸ“ˆ æ€§èƒ½æŒ‡æ¨™:")
    print(f"   RMSE: {performance.get('restoration_rmse', performance.get('diff_rmse', 'N/A')):.4f}â„ƒ")
    print(f"   RÂ²: {performance.get('restoration_r2', performance.get('diff_r2', 'N/A')):.4f}")
    print(f"   æ–¹å‘ç²¾åº¦: {performance.get('direction_accuracy', 'N/A'):.1f}%")

    # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯è­¦å‘Š
    leak_analysis = results['data_leak_analysis']
    if leak_analysis['potential_leaks']:
        print(f"\\nâš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯è­¦å‘Š:")
        print(f"   æ½œåœ¨çš„ãƒªã‚¹ã‚¯ç‰¹å¾´é‡: {len(leak_analysis['potential_leaks'])}å€‹")

    # æ”¹å–„ææ¡ˆ
    recommendations = results['improvement_recommendations']
    priority_actions = recommendations['priority_actions']
    if priority_actions:
        print(f"\\nğŸ¯ å„ªå…ˆæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        for i, action in enumerate(priority_actions, 1):
            print(f"   {i}. {action['issue']}")
            print(f"      â†’ {action['action']}")

    print("\\nâœ… åˆ†æå®Œäº†ï¼è©³ç´°ã¯ä¿å­˜ã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")


if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    zone = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    horizon = int(sys.argv[2]) if len(sys.argv) > 2 else 15

    # åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ
    results = create_comprehensive_analysis_report(zone, horizon)

    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    print_analysis_summary(results)

    print("\\nâœ… åŒ…æ‹¬çš„åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
