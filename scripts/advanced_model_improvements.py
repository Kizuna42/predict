#!/usr/bin/env python
# coding: utf-8

"""
é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åˆ†æçµæœã«åŸºã¥ã„ãŸæ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…:
1. æ™‚é–“ç›¸é–¢ã®æ”¹å–„ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç‰¹å¾´é‡ï¼‰
2. å¤–ã‚Œå€¤ã«å¯¾ã™ã‚‹å …ç‰¢ãªå­¦ç¿’
3. æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ˜ç¤ºçš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import sys
import os
from datetime import datetime
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

warnings.filterwarnings('ignore')

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import SMOOTHING_WINDOWS, FEATURE_SELECTION_THRESHOLD, TEST_SIZE

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.preprocessing import (
    filter_temperature_outliers,
    create_temperature_difference_targets,
    prepare_time_features,
    get_time_based_train_test_split,
    filter_high_value_targets
)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.feature_engineering import create_difference_prediction_pipeline

# è©•ä¾¡é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.evaluation import evaluate_temperature_difference_model


def create_advanced_temporal_features(df, zone_nums, horizons_minutes, time_diff_seconds):
    """
    é«˜åº¦ãªæ™‚é–“ç‰¹å¾´é‡ã®ä½œæˆ
    - ãƒ©ã‚°ç‰¹å¾´é‡
    - ç§»å‹•çµ±è¨ˆ
    - å‘¨æœŸæ€§ç‰¹å¾´é‡
    - æ™‚é–“çš„ç›¸äº’ä½œç”¨

    Parameters:
    -----------
    df : DataFrame
        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    zone_nums : list
        ã‚¾ãƒ¼ãƒ³ç•ªå·ã®ãƒªã‚¹ãƒˆ
    horizons_minutes : list
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
    time_diff_seconds : float
        ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”

    Returns:
    --------
    DataFrame, list
        æ‹¡å¼µã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨æ–°ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
    """
    print("\\nğŸ”¥ é«˜åº¦ãªæ™‚é–“ç‰¹å¾´é‡ã‚’ä½œæˆä¸­...")
    df_copy = df.copy()
    temporal_features = []

    # 1. ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆçŸ­æœŸã‹ã‚‰é•·æœŸã¾ã§ï¼‰
    lag_minutes = [5, 10, 15, 30, 60, 120]  # æ§˜ã€…ãªæ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«

    for zone in zone_nums:
        base_temp_col = f'sens_temp_{zone}'
        if base_temp_col in df_copy.columns:

            for lag_min in lag_minutes:
                lag_steps = int(lag_min * 60 / time_diff_seconds)

                # æ¸©åº¦ãƒ©ã‚°ç‰¹å¾´é‡
                lag_col = f'temp_lag_{zone}_{lag_min}min'
                df_copy[lag_col] = df_copy[base_temp_col].shift(lag_steps)
                temporal_features.append(lag_col)

                # æ¸©åº¦å¤‰åŒ–ãƒ©ã‚°ç‰¹å¾´é‡
                temp_change_col = f'temp_change_{zone}_{lag_min}min'
                if temp_change_col in df_copy.columns:
                    lag_change_col = f'temp_change_lag_{zone}_{lag_min}min'
                    df_copy[lag_change_col] = df_copy[temp_change_col].shift(lag_steps)
                    temporal_features.append(lag_change_col)

    # 2. ç§»å‹•çµ±è¨ˆç‰¹å¾´é‡ï¼ˆãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆï¼‰
    window_minutes = [15, 30, 60, 120]

    for zone in zone_nums:
        base_temp_col = f'sens_temp_{zone}'
        if base_temp_col in df_copy.columns:

            for window_min in window_minutes:
                window_steps = int(window_min * 60 / time_diff_seconds)

                # ç§»å‹•å¹³å‡ï¼ˆæ—¢å­˜ã®å¹³æ»‘åŒ–ã‚ˆã‚Šé•·æœŸï¼‰
                if window_min > 15:  # æ—¢å­˜ã®çŸ­æœŸå¹³æ»‘åŒ–ã¨é‡è¤‡ã‚’é¿ã‘ã‚‹
                    rolling_mean_col = f'temp_rolling_mean_{zone}_{window_min}min'
                    df_copy[rolling_mean_col] = df_copy[base_temp_col].rolling(
                        window=window_steps, min_periods=1, center=False
                    ).mean()
                    temporal_features.append(rolling_mean_col)

                # ç§»å‹•æ¨™æº–åå·®ï¼ˆå¤‰å‹•æ€§ã®æŒ‡æ¨™ï¼‰
                rolling_std_col = f'temp_rolling_std_{zone}_{window_min}min'
                df_copy[rolling_std_col] = df_copy[base_temp_col].rolling(
                    window=window_steps, min_periods=1, center=False
                ).std()
                temporal_features.append(rolling_std_col)

                # ç§»å‹•æœ€å¤§ãƒ»æœ€å°ï¼ˆç¯„å›²ã®æŒ‡æ¨™ï¼‰
                rolling_max_col = f'temp_rolling_max_{zone}_{window_min}min'
                rolling_min_col = f'temp_rolling_min_{zone}_{window_min}min'
                df_copy[rolling_max_col] = df_copy[base_temp_col].rolling(
                    window=window_steps, min_periods=1, center=False
                ).max()
                df_copy[rolling_min_col] = df_copy[base_temp_col].rolling(
                    window=window_steps, min_periods=1, center=False
                ).min()
                temporal_features.extend([rolling_max_col, rolling_min_col])

    # 3. å‘¨æœŸæ€§ç‰¹å¾´é‡ã®å¼·åŒ–
    if 'hour' in df_copy.columns:
        # æ™‚é–“å¸¯ã®æ¸©åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
        for zone in zone_nums:
            base_temp_col = f'sens_temp_{zone}'
            if base_temp_col in df_copy.columns:
                # æ™‚é–“åˆ¥å¹³å‡æ¸©åº¦ã‹ã‚‰ã®åå·®
                hourly_mean = df_copy.groupby('hour')[base_temp_col].transform('mean')
                temp_hour_deviation_col = f'temp_hour_deviation_{zone}'
                df_copy[temp_hour_deviation_col] = df_copy[base_temp_col] - hourly_mean
                temporal_features.append(temp_hour_deviation_col)

    # 4. æ™‚é–“çš„ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
    for zone in zone_nums:
        base_temp_col = f'sens_temp_{zone}'
        if base_temp_col in df_copy.columns and 'hour' in df_copy.columns:
            # æ™‚é–“ã¨æ¸©åº¦ã®ç›¸äº’ä½œç”¨
            temp_hour_interaction_col = f'temp_hour_interaction_{zone}'
            df_copy[temp_hour_interaction_col] = df_copy[base_temp_col] * df_copy['hour']
            temporal_features.append(temp_hour_interaction_col)

    # 5. ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ï¼ˆçŸ­æœŸãƒ»ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    trend_windows = [30, 60, 120]  # åˆ†

    for zone in zone_nums:
        base_temp_col = f'sens_temp_{zone}'
        if base_temp_col in df_copy.columns:

            for trend_min in trend_windows:
                trend_steps = int(trend_min * 60 / time_diff_seconds)

                # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæœ€å°äºŒä¹—æ³•ã®å‚¾ãï¼‰
                trend_col = f'temp_trend_{zone}_{trend_min}min'

                def calculate_trend(series):
                    if len(series) < 2 or series.isna().all():
                        return np.nan
                    x = np.arange(len(series))
                    y = series.values
                    valid_mask = ~np.isnan(y)
                    if valid_mask.sum() < 2:
                        return np.nan
                    return np.polyfit(x[valid_mask], y[valid_mask], 1)[0]

                df_copy[trend_col] = df_copy[base_temp_col].rolling(
                    window=trend_steps, min_periods=2, center=False
                ).apply(calculate_trend, raw=False)
                temporal_features.append(trend_col)

    print(f"âœ… é«˜åº¦ãªæ™‚é–“ç‰¹å¾´é‡ã‚’{len(temporal_features)}å€‹ä½œæˆã—ã¾ã—ãŸ")
    return df_copy, temporal_features


def create_robust_training_weights(y_train, residual_threshold_factor=2.0):
    """
    å¤–ã‚Œå€¤ã«å¯¾ã™ã‚‹å …ç‰¢ãªé‡ã¿ä»˜ã‘ã®ä½œæˆ

    Parameters:
    -----------
    y_train : Series
        è¨“ç·´ç”¨ç›®çš„å¤‰æ•°
    residual_threshold_factor : float
        å¤–ã‚Œå€¤åˆ¤å®šã®é–¾å€¤å› å­

    Returns:
    --------
    np.array
        ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é‡ã¿
    """
    print("\\nğŸ¯ å …ç‰¢ãªé‡ã¿ä»˜ã‘æˆ¦ç•¥ã‚’ä½œæˆä¸­...")

    # åŸºæœ¬çµ±è¨ˆ
    y_std = y_train.std()
    y_median = y_train.median()

    # 1. åŸºæœ¬é‡ã¿ï¼ˆæ—¢å­˜ã®é‡ã¿ä»˜ã‘æˆ¦ç•¥ï¼‰
    abs_values = y_train.abs()

    # é«˜ã„å¤‰åŒ–ã«é‡ã¿
    high_change_weight = np.where(abs_values > abs_values.quantile(0.8), 2.0, 1.0)

    # æ¥µå°å¤‰åŒ–ã®æ¤œå‡ºèƒ½åŠ›å‘ä¸Š
    tiny_change_weight = np.where(abs_values < abs_values.quantile(0.1), 1.5, 1.0)

    # æ–¹å‘è»¢æ›ç‚¹ã®é‡è¦æ€§
    direction_change_weight = np.ones(len(y_train))
    if len(y_train) > 2:
        direction_changes = ((y_train.shift(1) > 0) & (y_train < 0)) | ((y_train.shift(1) < 0) & (y_train > 0))
        direction_change_weight = np.where(direction_changes, 2.0, 1.0)

    # 2. å¤–ã‚Œå€¤æŠ‘åˆ¶é‡ã¿
    outlier_threshold = residual_threshold_factor * y_std
    outlier_weight = np.where(abs_values > outlier_threshold, 0.5, 1.0)  # å¤–ã‚Œå€¤ã®é‡ã¿ã‚’ä¸‹ã’ã‚‹

    # 3. æ™‚é–“çš„è¿‘æ¥æ€§é‡ã¿ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã«é«˜ã„é‡ã¿ï¼‰
    temporal_weight = np.linspace(0.8, 1.2, len(y_train))  # æœ€æ–°20%é‡ã¿å¢—

    # 4. çµ±åˆé‡ã¿è¨ˆç®—
    combined_weights = (high_change_weight * tiny_change_weight *
                       direction_change_weight * outlier_weight * temporal_weight)

    # é‡ã¿ã®æ­£è¦åŒ–ï¼ˆå¹³å‡1.0ã«ãªã‚‹ã‚ˆã†ã«ï¼‰
    combined_weights = combined_weights / combined_weights.mean()

    # é‡ã¿ã®ç¯„å›²åˆ¶é™ï¼ˆ0.2ï½5.0ï¼‰
    combined_weights = np.clip(combined_weights, 0.2, 5.0)

    print(f"âœ… å …ç‰¢ãªé‡ã¿ä»˜ã‘å®Œäº†:")
    print(f"   å¹³å‡é‡ã¿: {combined_weights.mean():.3f}")
    print(f"   é‡ã¿ç¯„å›²: {combined_weights.min():.3f} - {combined_weights.max():.3f}")
    print(f"   å¤–ã‚Œå€¤æŠ‘åˆ¶å¯¾è±¡: {(abs_values > outlier_threshold).sum()}è¡Œ ({(abs_values > outlier_threshold).mean():.1%})")

    return combined_weights


def train_advanced_temperature_difference_model(X_train, y_train, sample_weights=None):
    """
    é«˜åº¦ãªæ¸©åº¦å·®åˆ†äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

    Parameters:
    -----------
    X_train : DataFrame
        è¨“ç·´ç”¨ç‰¹å¾´é‡
    y_train : Series
        è¨“ç·´ç”¨ç›®çš„å¤‰æ•°
    sample_weights : array, optional
        ã‚µãƒ³ãƒ—ãƒ«é‡ã¿

    Returns:
    --------
    lgb.LGBMRegressor
        è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    print("\\nğŸ”¥ é«˜åº¦ãªæ¸©åº¦å·®åˆ†äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ”¹å–„ç‰ˆï¼‰
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 127,  # ã‚„ã‚„å¢—åŠ ï¼ˆè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        'learning_rate': 0.05,  # ã‚„ã‚„ä½ä¸‹ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_child_samples': 20,
        'min_child_weight': 0.001,
        'reg_alpha': 0.1,  # L1æ­£å‰‡åŒ–ï¼ˆç‰¹å¾´é¸æŠï¼‰
        'reg_lambda': 0.1,  # L2æ­£å‰‡åŒ–ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
        'max_depth': 8,
        'verbosity': -1,
        'random_state': 42,
        'n_jobs': -1,

        # å¤–ã‚Œå€¤ã«å¯¾ã™ã‚‹å …ç‰¢æ€§å‘ä¸Š
        'extra_trees': True,  # ãƒ©ãƒ³ãƒ€ãƒ æ€§è¿½åŠ 
        'path_smooth': 1.0,   # ãƒ‘ã‚¹å¹³æ»‘åŒ–
    }

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨è¨“ç·´
    model = lgb.LGBMRegressor(**lgb_params, n_estimators=2000)

    # æ—©æœŸåœæ­¢ç”¨ã®æ¤œè¨¼ã‚»ãƒƒãƒˆä½œæˆ
    X_train_split = X_train.iloc[:-1000]
    X_val_split = X_train.iloc[-1000:]
    y_train_split = y_train.iloc[:-1000]
    y_val_split = y_train.iloc[-1000:]

    if sample_weights is not None:
        weights_train_split = sample_weights[:-1000]
        weights_val_split = sample_weights[-1000:]
    else:
        weights_train_split = None
        weights_val_split = None

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model.fit(
        X_train_split, y_train_split,
        sample_weight=weights_train_split,
        eval_set=[(X_val_split, y_val_split)],
        eval_sample_weight=[weights_val_split] if weights_val_split is not None else None,
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=False),
            lgb.log_evaluation(0)  # ãƒ­ã‚°å‡ºåŠ›ç„¡åŠ¹åŒ–
        ]
    )

    print(f"âœ… é«˜åº¦ãªå·®åˆ†äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† (æœ€çµ‚ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {model.best_iteration_})")

    return model


def compare_models(original_results, advanced_results, zone, horizon):
    """
    ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ”¹å–„ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ

    Parameters:
    -----------
    original_results : dict
        ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®çµæœ
    advanced_results : dict
        æ”¹å–„ãƒ¢ãƒ‡ãƒ«ã®çµæœ
    zone : int
        ã‚¾ãƒ¼ãƒ³ç•ªå·
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³

    Returns:
    --------
    dict
        æ¯”è¼ƒçµæœ
    """
    print(f"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒåˆ†æ (ã‚¾ãƒ¼ãƒ³{zone}, {horizon}åˆ†äºˆæ¸¬)")
    print("="*60)

    comparison = {}

    # ä¸»è¦æŒ‡æ¨™ã®æ¯”è¼ƒ
    metrics_to_compare = [
        ('restoration_rmse', 'RMSE (â„ƒ)'),
        ('restoration_r2', 'RÂ²'),
        ('direction_accuracy', 'æ–¹å‘ç²¾åº¦ (%)'),
        ('restoration_mae', 'MAE (â„ƒ)')
    ]

    for metric_key, metric_name in metrics_to_compare:
        original_val = original_results.get(metric_key, np.nan)
        advanced_val = advanced_results.get(metric_key, np.nan)

        if not np.isnan(original_val) and not np.isnan(advanced_val):
            if metric_key in ['restoration_rmse', 'restoration_mae']:
                # å°ã•ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™
                improvement = (original_val - advanced_val) / original_val * 100
                comparison[metric_key] = {
                    'original': original_val,
                    'advanced': advanced_val,
                    'improvement_pct': improvement,
                    'better': advanced_val < original_val
                }
            else:
                # å¤§ãã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™
                improvement = (advanced_val - original_val) / original_val * 100
                comparison[metric_key] = {
                    'original': original_val,
                    'advanced': advanced_val,
                    'improvement_pct': improvement,
                    'better': advanced_val > original_val
                }

            print(f"{metric_name}:")
            print(f"  ã‚ªãƒªã‚¸ãƒŠãƒ«: {original_val:.4f}")
            print(f"  æ”¹å–„ç‰ˆ:     {advanced_val:.4f}")
            print(f"  æ”¹å–„ç‡:     {improvement:+.2f}%")
            print(f"  çµæœ:       {'âœ… æ”¹å–„' if comparison[metric_key]['better'] else 'âŒ æ‚ªåŒ–'}")
            print()

    return comparison


def create_advanced_model_analysis(zone=1, horizon=15):
    """
    æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„åˆ†æ

    Parameters:
    -----------
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³

    Returns:
    --------
    dict
        åˆ†æçµæœ
    """
    print("ğŸš€ é«˜åº¦ãªæ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«åˆ†æé–‹å§‹")
    print(f"å¯¾è±¡: ã‚¾ãƒ¼ãƒ³{zone}, äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³{horizon}åˆ†")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜å‰å‡¦ç†ï¼‰
    data_path = project_root / "AllDayData.csv"
    print(f"\\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")

    df = pd.read_csv(data_path, low_memory=False)

    # æ™‚é–“åˆ—ã®è¨­å®š
    if 'time_stamp' in df.columns:
        df['datetime'] = pd.to_datetime(df['time_stamp'])
        df = df.set_index('datetime')

    time_diff_seconds = df.index.to_series().diff().dropna().value_counts().index[0].total_seconds()

    # åŸºæœ¬å‰å‡¦ç†
    df = filter_temperature_outliers(df)
    df = prepare_time_features(df)

    # å·®åˆ†äºˆæ¸¬ç”¨ç›®çš„å¤‰æ•°ã®ä½œæˆ
    df_with_diff_targets = create_temperature_difference_targets(
        df, [zone], [horizon], pd.Timedelta(seconds=time_diff_seconds)
    )

    # åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    df_processed, selected_features, feature_info = create_difference_prediction_pipeline(
        df=df_with_diff_targets,
        zone_nums=[zone],
        horizons_minutes=[horizon],
        time_diff_seconds=time_diff_seconds,
        smoothing_window=SMOOTHING_WINDOWS[0] if SMOOTHING_WINDOWS else 5,
        feature_selection_threshold=FEATURE_SELECTION_THRESHOLD
    )

    # é«˜åº¦ãªæ™‚é–“ç‰¹å¾´é‡ã®è¿½åŠ 
    df_enhanced, temporal_features = create_advanced_temporal_features(
        df_processed, [zone], [horizon], time_diff_seconds
    )

    # å…¨ç‰¹å¾´é‡ã®çµ±åˆ
    all_features = selected_features + temporal_features
    all_features = [f for f in all_features if f in df_enhanced.columns]

    print(f"\\nğŸ“Š ç‰¹å¾´é‡çµ±è¨ˆ:")
    print(f"   åŸºæœ¬ç‰¹å¾´é‡: {len(selected_features)}")
    print(f"   æ™‚é–“ç‰¹å¾´é‡: {len(temporal_features)}")
    print(f"   ç·ç‰¹å¾´é‡æ•°: {len(all_features)}")

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    diff_target_col = f'temp_diff_{zone}_future_{horizon}'
    valid_data = df_enhanced.dropna(subset=[diff_target_col] + all_features)

    # é«˜å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–çµæœï¼š5%ileãŒæœ€é©ï¼‰
    # ä¸Šå¸è¦æ±‚ï¼šã€Œç›®çš„å¤‰æ•°ã®å€¤ãŒå¤§ãã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«çµã£ã¦å­¦ç¿’ã€
    abs_diff_col = f'abs_temp_diff_{zone}_future_{horizon}'
    valid_data[abs_diff_col] = valid_data[diff_target_col].abs()
    filtered_data, filter_info = filter_high_value_targets(
        valid_data, [abs_diff_col], percentile=5
    )

    # æ™‚ç³»åˆ—åˆ†å‰²
    train_df, test_df = get_time_based_train_test_split(filtered_data, test_size=TEST_SIZE)

    X_train = train_df[all_features]
    y_train_diff = train_df[diff_target_col]
    X_test = test_df[all_features]
    y_test_diff = test_df[diff_target_col]
    current_temps_test = test_df[f'sens_temp_{zone}']

    print(f"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]}è¡Œ x {X_train.shape[1]}ç‰¹å¾´é‡")
    print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]}è¡Œ")

    # å …ç‰¢ãªé‡ã¿ä»˜ã‘ã®ä½œæˆ
    robust_weights = create_robust_training_weights(y_train_diff)

    # æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´
    advanced_model = train_advanced_temperature_difference_model(
        X_train, y_train_diff, sample_weights=robust_weights
    )

    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred_advanced = advanced_model.predict(X_test)
    advanced_metrics = evaluate_temperature_difference_model(
        y_test_diff, y_pred_advanced, current_temps_test
    )

    # ã‚ªãƒªã‚¸ãƒŠãƒ«çµæœã®èª­ã¿è¾¼ã¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    original_analysis_file = project_root / "Output" / f"comprehensive_analysis_zone{zone}_horizon{horizon}.json"
    original_metrics = {}
    if original_analysis_file.exists():
        with open(original_analysis_file, 'r', encoding='utf-8') as f:
            original_analysis = json.load(f)
            original_metrics = original_analysis['performance_analysis']['basic_metrics']

    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    comparison_results = compare_models(original_metrics, advanced_metrics, zone, horizon)

    # çµæœçµ±åˆ
    advanced_results = {
        'metadata': {
            'zone': zone,
            'horizon': horizon,
            'analysis_timestamp': datetime.now().isoformat(),
            'model_type': 'advanced_temporal_robust',
            'feature_count': {
                'basic': len(selected_features),
                'temporal': len(temporal_features),
                'total': len(all_features)
            }
        },
        'advanced_metrics': advanced_metrics,
        'model_comparison': comparison_results,
        'feature_importance': {
            'top_20': [
                {
                    'feature': all_features[i],
                    'importance': float(importance)
                }
                for i, importance in enumerate(advanced_model.feature_importances_)
            ][:20]
        },
        'training_info': {
            'robust_weights_used': True,
            'temporal_features_added': len(temporal_features),
            'best_iteration': int(advanced_model.best_iteration_)
        }
    }

    # çµæœä¿å­˜
    output_dir = project_root / "Output"
    results_file = output_dir / f"advanced_model_analysis_zone{zone}_horizon{horizon}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(advanced_results, f, indent=2, ensure_ascii=False, default=str)

    print(f"\\nçµæœã‚’ä¿å­˜: {results_file}")

    return advanced_results


if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    zone = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    horizon = int(sys.argv[2]) if len(sys.argv) > 2 else 15

    # æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«åˆ†æå®Ÿè¡Œ
    results = create_advanced_model_analysis(zone, horizon)

    print("\\nğŸ‰ é«˜åº¦ãªæ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
