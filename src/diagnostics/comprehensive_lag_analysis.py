#!/usr/bin/env python
# coding: utf-8

"""
åŒ…æ‹¬çš„LAGåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œã®è©³ç´°è¨ºæ–­ã¨åŸå› ç‰¹å®š
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


def analyze_lag_following_comprehensive(timestamps: pd.DatetimeIndex,
                                      actual_values: np.ndarray,
                                      predicted_values: np.ndarray,
                                      feature_importance: pd.DataFrame,
                                      zone: int,
                                      horizon: int) -> Dict[str, Any]:
    """
    LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œã®åŒ…æ‹¬çš„åˆ†æ

    Parameters:
    -----------
    timestamps : pd.DatetimeIndex
        ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    actual_values : np.ndarray
        å®Ÿæ¸¬å€¤
    predicted_values : np.ndarray
        äºˆæ¸¬å€¤
    feature_importance : pd.DataFrame
        ç‰¹å¾´é‡é‡è¦åº¦
    zone : int
        ã‚¾ãƒ¼ãƒ³ç•ªå·
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆåˆ†ï¼‰

    Returns:
    --------
    dict
        åŒ…æ‹¬çš„åˆ†æçµæœ
    """

    analysis_results = {
        'zone': zone,
        'horizon': horizon,
        'timestamp_analysis': {},
        'pattern_analysis': {},
        'feature_analysis': {},
        'lag_dependency': {},
        'recommendations': [],
        'severity': 'low'
    }

    # 1. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æ
    analysis_results['timestamp_analysis'] = _analyze_timestamp_alignment(
        timestamps, actual_values, predicted_values, horizon
    )

    # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆè°·ã‚„å±±ã®å¾Œè¿½ã„æ¤œå‡ºï¼‰
    analysis_results['pattern_analysis'] = _analyze_pattern_following(
        actual_values, predicted_values, horizon
    )

    # 3. ç‰¹å¾´é‡åˆ†æ
    analysis_results['feature_analysis'] = _analyze_feature_dependency(
        feature_importance
    )

    # 4. LAGä¾å­˜åº¦åˆ†æ
    analysis_results['lag_dependency'] = _analyze_lag_dependency_detailed(
        feature_importance
    )

    # 5. ç·åˆè©•ä¾¡ã¨æ¨å¥¨äº‹é …
    analysis_results = _generate_comprehensive_recommendations(analysis_results)

    return analysis_results


def _analyze_timestamp_alignment(timestamps: pd.DatetimeIndex,
                               actual_values: np.ndarray,
                               predicted_values: np.ndarray,
                               horizon: int) -> Dict[str, Any]:
    """
    ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ•´åˆæ€§åˆ†æ
    """
    timestamp_analysis = {
        'is_correct_alignment': True,
        'detected_lag_minutes': 0,
        'correlation_by_lag': {},
        'issues': []
    }

    if len(actual_values) < 50:
        timestamp_analysis['issues'].append("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚åˆ†æä¸å¯")
        return timestamp_analysis

    # ç›¸äº’ç›¸é–¢åˆ†æ
    max_lag_steps = min(horizon // 5 + 10, len(actual_values) // 4)
    correlations = {}

    for lag in range(-max_lag_steps, max_lag_steps + 1):
        try:
            if lag < 0:
                corr = np.corrcoef(actual_values[-lag:], predicted_values[:lag])[0, 1]
            elif lag > 0:
                corr = np.corrcoef(actual_values[:-lag], predicted_values[lag:])[0, 1]
            else:
                corr = np.corrcoef(actual_values, predicted_values)[0, 1]

            if not np.isnan(corr):
                correlations[lag] = corr
        except:
            continue

    timestamp_analysis['correlation_by_lag'] = correlations

    if correlations:
        max_corr_lag = max(correlations.keys(), key=lambda k: abs(correlations[k]))
        max_corr_value = correlations[max_corr_lag]

        if max_corr_lag > 0 and abs(max_corr_value) > 0.8:
            timestamp_analysis['is_correct_alignment'] = False
            timestamp_analysis['detected_lag_minutes'] = max_corr_lag * 5
            timestamp_analysis['issues'].append(
                f"äºˆæ¸¬å€¤ãŒå®Ÿæ¸¬å€¤ã‚ˆã‚Š{max_corr_lag * 5}åˆ†é…ã‚Œã¦è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§"
            )

    return timestamp_analysis


def _analyze_pattern_following(actual_values: np.ndarray,
                             predicted_values: np.ndarray,
                             horizon: int) -> Dict[str, Any]:
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³å¾Œè¿½ã„ã®è©³ç´°åˆ†æï¼ˆè°·ã‚„å±±ã®æ¤œå‡ºï¼‰
    """
    pattern_analysis = {
        'valley_following': False,
        'peak_following': False,
        'pattern_lag_minutes': 0,
        'pattern_correlation': 0.0,
        'detected_patterns': []
    }

    if len(actual_values) < 100:
        return pattern_analysis

    # æ¥µå€¤æ¤œå‡ºï¼ˆè°·ã¨å±±ï¼‰
    actual_peaks = _detect_peaks_and_valleys(actual_values)
    predicted_peaks = _detect_peaks_and_valleys(predicted_values)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°åˆ†æ
    pattern_matches = _analyze_pattern_matches(actual_peaks, predicted_peaks, horizon)

    pattern_analysis.update(pattern_matches)

    return pattern_analysis


def _detect_peaks_and_valleys(values: np.ndarray, prominence=0.5) -> Dict[str, List[int]]:
    """
    æ¥µå€¤ï¼ˆå±±ã¨è°·ï¼‰ã®æ¤œå‡º
    """
    from scipy.signal import find_peaks

    # å±±ã®æ¤œå‡º
    peaks, _ = find_peaks(values, prominence=prominence)

    # è°·ã®æ¤œå‡ºï¼ˆå€¤ã‚’åè»¢ã—ã¦å±±ã‚’æ¤œå‡ºï¼‰
    valleys, _ = find_peaks(-values, prominence=prominence)

    return {
        'peaks': peaks.tolist(),
        'valleys': valleys.tolist()
    }


def _analyze_pattern_matches(actual_patterns: Dict[str, List[int]],
                           predicted_patterns: Dict[str, List[int]],
                           horizon: int) -> Dict[str, Any]:
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã®åˆ†æ
    """
    matches = {
        'valley_following': False,
        'peak_following': False,
        'pattern_lag_minutes': 0,
        'detected_patterns': []
    }

    # è°·ã®ãƒãƒƒãƒãƒ³ã‚°åˆ†æ
    valley_lag = _calculate_pattern_lag(
        actual_patterns['valleys'],
        predicted_patterns['valleys']
    )

    # å±±ã®ãƒãƒƒãƒãƒ³ã‚°åˆ†æ
    peak_lag = _calculate_pattern_lag(
        actual_patterns['peaks'],
        predicted_patterns['peaks']
    )

    if valley_lag > 0:
        matches['valley_following'] = True
        matches['pattern_lag_minutes'] = valley_lag * 5
        matches['detected_patterns'].append(f"è°·ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ{valley_lag * 5}åˆ†é…ã‚Œã¦å‡ºç¾")

    if peak_lag > 0:
        matches['peak_following'] = True
        matches['pattern_lag_minutes'] = max(matches['pattern_lag_minutes'], peak_lag * 5)
        matches['detected_patterns'].append(f"å±±ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ{peak_lag * 5}åˆ†é…ã‚Œã¦å‡ºç¾")

    return matches


def _calculate_pattern_lag(actual_indices: List[int],
                         predicted_indices: List[int]) -> int:
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é…ã‚Œã‚’è¨ˆç®—
    """
    if not actual_indices or not predicted_indices:
        return 0

    min_lag = float('inf')

    for actual_idx in actual_indices:
        for pred_idx in predicted_indices:
            lag = pred_idx - actual_idx
            if 0 < lag < min_lag:
                min_lag = lag

    return min_lag if min_lag != float('inf') else 0


def _analyze_feature_dependency(feature_importance: pd.DataFrame) -> Dict[str, Any]:
    """
    ç‰¹å¾´é‡ä¾å­˜åº¦ã®è©³ç´°åˆ†æ
    """
    feature_analysis = {
        'total_features': len(feature_importance),
        'lag_features': [],
        'smoothed_features': [],
        'future_features': [],
        'current_temp_features': [],
        'other_features': [],
        'dependency_percentages': {}
    }

    total_importance = feature_importance['importance'].sum()

    if total_importance == 0:
        return feature_analysis

    # ç‰¹å¾´é‡ã®åˆ†é¡
    for _, row in feature_importance.iterrows():
        feature_name = row['feature']
        importance = row['importance']

        if '_lag_' in feature_name:
            feature_analysis['lag_features'].append({
                'name': feature_name,
                'importance': importance,
                'percentage': (importance / total_importance) * 100
            })
        elif 'smoothed' in feature_name or 'rolling' in feature_name:
            feature_analysis['smoothed_features'].append({
                'name': feature_name,
                'importance': importance,
                'percentage': (importance / total_importance) * 100
            })
        elif 'future' in feature_name:
            feature_analysis['future_features'].append({
                'name': feature_name,
                'importance': importance,
                'percentage': (importance / total_importance) * 100
            })
        elif 'sens_temp' in feature_name and 'lag' not in feature_name and 'future' not in feature_name:
            feature_analysis['current_temp_features'].append({
                'name': feature_name,
                'importance': importance,
                'percentage': (importance / total_importance) * 100
            })
        else:
            feature_analysis['other_features'].append({
                'name': feature_name,
                'importance': importance,
                'percentage': (importance / total_importance) * 100
            })

    # ä¾å­˜åº¦ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã®è¨ˆç®—
    feature_analysis['dependency_percentages'] = {
        'lag_dependency': sum([f['percentage'] for f in feature_analysis['lag_features']]),
        'smoothed_dependency': sum([f['percentage'] for f in feature_analysis['smoothed_features']]),
        'future_dependency': sum([f['percentage'] for f in feature_analysis['future_features']]),
        'current_temp_dependency': sum([f['percentage'] for f in feature_analysis['current_temp_features']]),
        'other_dependency': sum([f['percentage'] for f in feature_analysis['other_features']])
    }

    return feature_analysis


def _analyze_lag_dependency_detailed(feature_importance: pd.DataFrame) -> Dict[str, Any]:
    """
    LAGä¾å­˜åº¦ã®è©³ç´°åˆ†æ
    """
    lag_analysis = {
        'has_explicit_lag_features': False,
        'implicit_lag_sources': [],
        'total_lag_like_dependency': 0.0,
        'risk_level': 'low'
    }

    total_importance = feature_importance['importance'].sum()

    if total_importance == 0:
        return lag_analysis

    # æ˜ç¤ºçš„ãªLAGç‰¹å¾´é‡ã®ç¢ºèª
    explicit_lag_features = feature_importance[
        feature_importance['feature'].str.contains('_lag_', na=False)
    ]

    if len(explicit_lag_features) > 0:
        lag_analysis['has_explicit_lag_features'] = True
        lag_analysis['total_lag_like_dependency'] += (
            explicit_lag_features['importance'].sum() / total_importance * 100
        )

    # æš—é»™çš„ãªLAGåŠ¹æœã‚’æŒã¤ç‰¹å¾´é‡ã®ç¢ºèª
    implicit_lag_patterns = [
        ('smoothed', 'å¹³æ»‘åŒ–ç‰¹å¾´é‡ï¼ˆéå»ã®å€¤ã®ç§»å‹•å¹³å‡ï¼‰'),
        ('rolling', 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆé‡ï¼ˆéå»ã®å€¤ã®çµ±è¨ˆï¼‰'),
        ('rate', 'å¤‰åŒ–ç‡ï¼ˆå‰ã®æ™‚ç‚¹ã¨ã®å·®åˆ†ï¼‰'),
        ('diff', 'å·®åˆ†ç‰¹å¾´é‡ï¼ˆå‰ã®æ™‚ç‚¹ã¨ã®å·®ï¼‰')
    ]

    for pattern, description in implicit_lag_patterns:
        matching_features = feature_importance[
            feature_importance['feature'].str.contains(pattern, na=False)
        ]

        if len(matching_features) > 0:
            dependency = matching_features['importance'].sum() / total_importance * 100
            lag_analysis['implicit_lag_sources'].append({
                'pattern': pattern,
                'description': description,
                'dependency_percentage': dependency,
                'feature_count': len(matching_features)
            })
            lag_analysis['total_lag_like_dependency'] += dependency

    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
    if lag_analysis['total_lag_like_dependency'] > 50:
        lag_analysis['risk_level'] = 'high'
    elif lag_analysis['total_lag_like_dependency'] > 30:
        lag_analysis['risk_level'] = 'medium'
    else:
        lag_analysis['risk_level'] = 'low'

    return lag_analysis


def _generate_comprehensive_recommendations(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    åŒ…æ‹¬çš„ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ
    """
    recommendations = []
    severity = 'low'

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å•é¡Œã®ç¢ºèª
    if not analysis_results['timestamp_analysis']['is_correct_alignment']:
        recommendations.append({
            'category': 'visualization',
            'priority': 'high',
            'issue': 'æ™‚é–“è»¸è¡¨ç¤ºã®å•é¡Œ',
            'description': 'äºˆæ¸¬å€¤ãŒé–“é•ã£ãŸæ™‚é–“è»¸ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™',
            'action': 'äºˆæ¸¬å€¤ã®è¡¨ç¤ºæ™‚åˆ»ã‚’ã€Œå…¥åŠ›æ™‚åˆ» + äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã€ã«ä¿®æ­£ã—ã¦ãã ã•ã„'
        })
        severity = 'high'

    # ãƒ‘ã‚¿ãƒ¼ãƒ³å¾Œè¿½ã„ã®ç¢ºèª
    pattern_analysis = analysis_results['pattern_analysis']
    if pattern_analysis['valley_following'] or pattern_analysis['peak_following']:
        recommendations.append({
            'category': 'model',
            'priority': 'high',
            'issue': 'ãƒ‘ã‚¿ãƒ¼ãƒ³å¾Œè¿½ã„',
            'description': f"å®Ÿæ¸¬å€¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ{pattern_analysis['pattern_lag_minutes']}åˆ†é…ã‚Œã¦äºˆæ¸¬ã«ç¾ã‚Œã¦ã„ã¾ã™",
            'action': 'LAGç‰¹å¾´é‡ã¸ã®ä¾å­˜åº¦ã‚’ä¸‹ã’ã€æœªæ¥æƒ…å ±ã®æ´»ç”¨ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„'
        })
        severity = 'high'

    # LAGä¾å­˜åº¦ã®ç¢ºèª
    lag_dependency = analysis_results['lag_dependency']
    if lag_dependency['risk_level'] == 'high':
        recommendations.append({
            'category': 'features',
            'priority': 'high',
            'issue': 'é«˜LAGä¾å­˜åº¦',
            'description': f"LAGæ§˜ç‰¹å¾´é‡ã¸ã®ä¾å­˜åº¦ãŒ{lag_dependency['total_lag_like_dependency']:.1f}%ã¨é«˜ã™ãã¾ã™",
            'action': 'ç‰©ç†æ³•å‰‡ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚„æœªæ¥æƒ…å ±ã®æ¯”é‡ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„'
        })
        severity = 'high'
    elif lag_dependency['risk_level'] == 'medium':
        recommendations.append({
            'category': 'features',
            'priority': 'medium',
            'issue': 'ä¸­ç¨‹åº¦LAGä¾å­˜åº¦',
            'description': f"LAGæ§˜ç‰¹å¾´é‡ã¸ã®ä¾å­˜åº¦ãŒ{lag_dependency['total_lag_like_dependency']:.1f}%ã§ã™",
            'action': 'ç‰¹å¾´é‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„'
        })
        if severity == 'low':
            severity = 'medium'

    # ç‰¹å¾´é‡ãƒãƒ©ãƒ³ã‚¹ã®ç¢ºèª
    feature_analysis = analysis_results['feature_analysis']
    future_dependency = feature_analysis['dependency_percentages'].get('future_dependency', 0)
    current_temp_dependency = feature_analysis['dependency_percentages'].get('current_temp_dependency', 0)

    if future_dependency < 20:
        recommendations.append({
            'category': 'features',
            'priority': 'medium',
            'issue': 'æœªæ¥æƒ…å ±ã®æ´»ç”¨ä¸è¶³',
            'description': f"æœªæ¥ç‰¹å¾´é‡ã¸ã®ä¾å­˜åº¦ãŒ{future_dependency:.1f}%ã¨ä½ã„ã§ã™",
            'action': 'åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã®æœªæ¥æƒ…å ±ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„'
        })

    if current_temp_dependency > 40:
        recommendations.append({
            'category': 'features',
            'priority': 'medium',
            'issue': 'ç¾åœ¨æ¸©åº¦ã¸ã®éåº¦ãªä¾å­˜',
            'description': f"ç¾åœ¨æ¸©åº¦ç‰¹å¾´é‡ã¸ã®ä¾å­˜åº¦ãŒ{current_temp_dependency:.1f}%ã¨é«˜ã„ã§ã™",
            'action': 'ä»–ã®ç‰©ç†çš„ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’é«˜ã‚ã¦ãã ã•ã„'
        })

    analysis_results['recommendations'] = recommendations
    analysis_results['severity'] = severity

    return analysis_results


def generate_lag_analysis_report(results_dict: Dict, horizon: int, save_dir: str = None) -> Dict[str, Any]:
    """
    å…¨ã‚¾ãƒ¼ãƒ³ã®LAGåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    report = {
        'horizon': horizon,
        'zones_analyzed': 0,
        'high_risk_zones': [],
        'medium_risk_zones': [],
        'low_risk_zones': [],
        'common_issues': [],
        'overall_recommendations': []
    }

    zone_analyses = {}

    for zone, zone_results in results_dict.items():
        if horizon not in zone_results:
            continue

        horizon_results = zone_results[horizon]

        # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        required_keys = ['test_data', 'test_y', 'test_predictions', 'feature_importance']
        if not all(k in horizon_results for k in required_keys):
            continue

        # ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        test_df = horizon_results['test_data']
        if not isinstance(test_df, pd.DataFrame) or not hasattr(test_df, 'index'):
            continue

        timestamps = test_df.index
        actual = horizon_results['test_y'].values
        predicted = horizon_results['test_predictions']
        feature_importance = horizon_results['feature_importance']

        # åŒ…æ‹¬çš„åˆ†æã®å®Ÿè¡Œ
        analysis = analyze_lag_following_comprehensive(
            timestamps, actual, predicted, feature_importance, zone, horizon
        )

        zone_analyses[zone] = analysis
        report['zones_analyzed'] += 1

        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥ã®åˆ†é¡
        if analysis['severity'] == 'high':
            report['high_risk_zones'].append(zone)
        elif analysis['severity'] == 'medium':
            report['medium_risk_zones'].append(zone)
        else:
            report['low_risk_zones'].append(zone)

    # å…±é€šå•é¡Œã®ç‰¹å®š
    report['common_issues'] = _identify_common_issues(zone_analyses)

    # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
    report['overall_recommendations'] = _generate_overall_recommendations(zone_analyses)

    # ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
    if save_dir:
        _save_lag_analysis_report(report, zone_analyses, save_dir, horizon)

    return report


def _identify_common_issues(zone_analyses: Dict) -> List[Dict[str, Any]]:
    """
    å…±é€šå•é¡Œã®ç‰¹å®š
    """
    common_issues = []

    # å„å•é¡Œã‚«ãƒ†ã‚´ãƒªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
    issue_counts = {}
    total_zones = len(zone_analyses)

    for zone, analysis in zone_analyses.items():
        for rec in analysis['recommendations']:
            category = rec['category']
            issue = rec['issue']
            key = f"{category}_{issue}"

            if key not in issue_counts:
                issue_counts[key] = {
                    'category': category,
                    'issue': issue,
                    'count': 0,
                    'zones': []
                }

            issue_counts[key]['count'] += 1
            issue_counts[key]['zones'].append(zone)

    # 50%ä»¥ä¸Šã®ã‚¾ãƒ¼ãƒ³ã§ç™ºç”Ÿã—ã¦ã„ã‚‹å•é¡Œã‚’å…±é€šå•é¡Œã¨ã™ã‚‹
    for key, data in issue_counts.items():
        if data['count'] / total_zones >= 0.5:
            common_issues.append({
                'category': data['category'],
                'issue': data['issue'],
                'affected_zones': data['zones'],
                'frequency': data['count'] / total_zones
            })

    return common_issues


def _generate_overall_recommendations(zone_analyses: Dict) -> List[str]:
    """
    å…¨ä½“çš„ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ
    """
    recommendations = []

    high_risk_count = sum(1 for analysis in zone_analyses.values() if analysis['severity'] == 'high')
    total_zones = len(zone_analyses)

    if high_risk_count / total_zones > 0.3:
        recommendations.append(
            "âš ï¸ é«˜ãƒªã‚¹ã‚¯ã‚¾ãƒ¼ãƒ³ãŒ30%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æˆ¦ç•¥ã®å…¨é¢çš„ãªè¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚"
        )

    # LAGä¾å­˜åº¦ã®å…¨ä½“çš„ãªå‚¾å‘
    high_lag_zones = sum(1 for analysis in zone_analyses.values()
                        if analysis['lag_dependency']['risk_level'] == 'high')

    if high_lag_zones > 0:
        recommendations.append(
            f"ğŸ”„ {high_lag_zones}å€‹ã®ã‚¾ãƒ¼ãƒ³ã§é«˜LAGä¾å­˜åº¦ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æœªæ¥æƒ…å ±ã®æ´»ç”¨ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚"
        )

    # æ™‚é–“è»¸å•é¡Œã®ç¢ºèª
    timestamp_issues = sum(1 for analysis in zone_analyses.values()
                          if not analysis['timestamp_analysis']['is_correct_alignment'])

    if timestamp_issues > 0:
        recommendations.append(
            f"ğŸ“… {timestamp_issues}å€‹ã®ã‚¾ãƒ¼ãƒ³ã§æ™‚é–“è»¸è¡¨ç¤ºã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å¯è¦–åŒ–ã®ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚"
        )

    return recommendations


def _save_lag_analysis_report(report: Dict, zone_analyses: Dict, save_dir: str, horizon: int):
    """
    LAGåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
    """
    import os
    import json

    os.makedirs(save_dir, exist_ok=True)

    # JSONãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
    report_path = os.path.join(save_dir, f'lag_analysis_report_horizon_{horizon}.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({
            'report': report,
            'zone_analyses': zone_analyses
        }, f, ensure_ascii=False, indent=2, default=str)

    print(f"LAGåˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
