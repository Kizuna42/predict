#!/usr/bin/env python
# coding: utf-8

"""
ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèªã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’çµ±ä¸€
"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Dict, Optional


def check_and_remove_duplicate_columns(df: pd.DataFrame, feature_names: Optional[List[str]] = None) -> Tuple[pd.DataFrame, List[str], Dict[str, int]]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åé‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€é‡è¤‡ã‚’æ’é™¤
    
    Parameters:
    -----------
    df : DataFrame
        å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    feature_names : list, optional
        ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ
        
    Returns:
    --------
    df_clean : DataFrame
        é‡è¤‡æ’é™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    clean_feature_names : list
        é‡è¤‡æ’é™¤å¾Œã®ç‰¹å¾´é‡å
    stats : dict
        å‡¦ç†çµ±è¨ˆæƒ…å ±
    """
    stats = {
        'original_columns': len(df.columns),
        'duplicates_removed': 0,
        'final_columns': 0
    }
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åé‡è¤‡ãƒã‚§ãƒƒã‚¯
    if len(df.columns) != len(set(df.columns)):
        print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åã«é‡è¤‡ã‚’æ¤œå‡º - é‡è¤‡ã‚’æ’é™¤ã—ã¾ã™")
        
        unique_cols = []
        seen_cols = set()
        
        for col in df.columns:
            if col not in seen_cols:
                unique_cols.append(col)
                seen_cols.add(col)
        
        duplicate_count = len(df.columns) - len(unique_cols)
        stats['duplicates_removed'] = duplicate_count
        
        df_clean = df[unique_cols]
        print(f"âœ… {duplicate_count}å€‹ã®é‡è¤‡åˆ—ã‚’æ’é™¤ (æ®‹ã‚Š: {len(unique_cols)}åˆ—)")
    else:
        df_clean = df.copy()
    
    # ç‰¹å¾´é‡åãƒªã‚¹ãƒˆã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    if feature_names is not None:
        unique_features = []
        seen_features = set()
        
        for feature in feature_names:
            if feature not in seen_features:
                unique_features.append(feature)
                seen_features.add(feature)
        
        if len(unique_features) != len(feature_names):
            feature_duplicates = len(feature_names) - len(unique_features)
            print(f"âš ï¸  ç‰¹å¾´é‡åãƒªã‚¹ãƒˆã®é‡è¤‡ã‚’æ’é™¤: {feature_duplicates}å€‹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ä¿æŒ
        clean_feature_names = [f for f in unique_features if f in df_clean.columns]
    else:
        clean_feature_names = list(df_clean.columns)
    
    stats['final_columns'] = len(df_clean.columns)
    return df_clean, clean_feature_names, stats


def validate_data_quality(df: pd.DataFrame, target_cols: List[str], min_rows: int = 100) -> Dict[str, any]:
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªã®åŸºæœ¬çš„ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    
    Parameters:
    -----------
    df : DataFrame
        å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    target_cols : list
        å¿…é ˆåˆ—ã®ãƒªã‚¹ãƒˆ
    min_rows : int
        æœ€å°å¿…è¦è¡Œæ•°
        
    Returns:
    --------
    validation_result : dict
        ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
    """
    result = {
        'is_valid': True,
        'warnings': [],
        'errors': [],
        'stats': {
            'total_rows': len(df),
            'missing_cols': [],
            'null_percentages': {}
        }
    }
    
    # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
    if len(df) < min_rows:
        result['errors'].append(f"ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ä¸è¶³: {len(df)}è¡Œ (æœ€å°å¿…è¦: {min_rows}è¡Œ)")
        result['is_valid'] = False
    
    # å¿…é ˆåˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    missing_cols = [col for col in target_cols if col not in df.columns]
    if missing_cols:
        result['errors'].append(f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_cols}")
        result['stats']['missing_cols'] = missing_cols
        result['is_valid'] = False
    
    # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    for col in target_cols:
        if col in df.columns:
            null_pct = df[col].isnull().mean() * 100
            result['stats']['null_percentages'][col] = null_pct
            
            if null_pct > 50:
                result['warnings'].append(f"{col}: æ¬ æå€¤ãŒ50%ä»¥ä¸Š ({null_pct:.1f}%)")
            elif null_pct > 20:
                result['warnings'].append(f"{col}: æ¬ æå€¤ãŒ20%ä»¥ä¸Š ({null_pct:.1f}%)")
    
    # ç„¡é™å€¤ãƒ»ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col in target_cols:
            inf_count = np.isinf(df[col]).sum()
            if inf_count > 0:
                result['warnings'].append(f"{col}: ç„¡é™å€¤ã‚’æ¤œå‡º ({inf_count}å€‹)")
    
    return result


def safe_data_preparation(df: pd.DataFrame, feature_cols: List[str], target_col: str) -> Tuple[pd.DataFrame, Dict[str, any]]:
    """
    å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿æº–å‚™ - é‡è¤‡é™¤å»ã¨å“è³ªãƒã‚§ãƒƒã‚¯ã‚’çµ±åˆ
    
    Parameters:
    -----------
    df : DataFrame
        å…ƒãƒ‡ãƒ¼ã‚¿
    feature_cols : list
        ç‰¹å¾´é‡åˆ—å
    target_col : str
        ç›®çš„å¤‰æ•°åˆ—å
        
    Returns:
    --------
    clean_data : DataFrame
        ã‚¯ãƒªãƒ¼ãƒ³æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    preparation_info : dict
        æº–å‚™å‡¦ç†ã®æƒ…å ±
    """
    preparation_info = {
        'original_shape': df.shape,
        'duplicate_removal': {},
        'validation': {},
        'final_shape': None
    }
    
    # é‡è¤‡é™¤å»
    df_clean, clean_features, dup_stats = check_and_remove_duplicate_columns(df, feature_cols)
    preparation_info['duplicate_removal'] = dup_stats
    
    # å¿…è¦ãªåˆ—ã®ã¿æŠ½å‡º
    required_cols = [col for col in clean_features + [target_col] if col in df_clean.columns]
    df_subset = df_clean[required_cols]
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    validation_result = validate_data_quality(df_subset, required_cols)
    preparation_info['validation'] = validation_result
    
    # æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
    df_final = df_subset.dropna(subset=required_cols)
    preparation_info['final_shape'] = df_final.shape
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {preparation_info['original_shape']} â†’ {preparation_info['final_shape']}")
    
    return df_final, preparation_info


def print_data_preparation_summary(preparation_info: Dict[str, any]) -> None:
    """
    ãƒ‡ãƒ¼ã‚¿æº–å‚™å‡¦ç†ã®çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    
    Parameters:
    -----------
    preparation_info : dict
        æº–å‚™å‡¦ç†ã®æƒ…å ±
    """
    print("\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚µãƒãƒªãƒ¼:")
    print(f"  ğŸ“¥ å…ƒãƒ‡ãƒ¼ã‚¿: {preparation_info['original_shape'][0]:,}è¡Œ x {preparation_info['original_shape'][1]}åˆ—")
    
    if preparation_info['duplicate_removal']['duplicates_removed'] > 0:
        print(f"  ğŸ”§ é‡è¤‡é™¤å»: {preparation_info['duplicate_removal']['duplicates_removed']}åˆ—")
    
    if preparation_info['final_shape']:
        print(f"  ğŸ“¤ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {preparation_info['final_shape'][0]:,}è¡Œ x {preparation_info['final_shape'][1]}åˆ—")
        
        row_reduction = preparation_info['original_shape'][0] - preparation_info['final_shape'][0]
        if row_reduction > 0:
            reduction_pct = (row_reduction / preparation_info['original_shape'][0]) * 100
            print(f"  ğŸ“‰ è¡Œæ•°å‰Šæ¸›: {row_reduction:,}è¡Œ ({reduction_pct:.1f}%)")
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
    validation = preparation_info['validation']
    if validation['warnings']:
        print(f"  âš ï¸  è­¦å‘Š: {len(validation['warnings'])}ä»¶")
    if validation['errors']:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {len(validation['errors'])}ä»¶")
    if validation['is_valid']:
        print(f"  âœ… ãƒ‡ãƒ¼ã‚¿å“è³ª: è‰¯å¥½") 