#!/usr/bin/env python
# coding: utf-8

"""
ç´°ã‹ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
60%ä»¥ä¸‹ã®ç¯„å›²ã§ã‚ˆã‚Šç´°ã‹ã„åˆ»ã¿ã§æœ€é©ãªé–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import time

warnings.filterwarnings('ignore')

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import SMOOTHING_WINDOWS, FEATURE_SELECTION_THRESHOLD, TEST_SIZE

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.preprocessing import (
    filter_temperature_outliers,
    create_temperature_difference_targets,
    prepare_time_features,
    get_time_based_train_test_split,
    filter_high_value_targets
)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.feature_engineering import create_difference_prediction_pipeline

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.training import train_temperature_difference_model

# è©•ä¾¡é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.evaluation import evaluate_temperature_difference_model


def test_filtering_threshold_fine(df, zone, horizon, percentile, time_diff_seconds):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€æ€§èƒ½ã‚’è©•ä¾¡ï¼ˆç´°ã‹ã„æœ€é©åŒ–ç”¨ï¼‰

    Parameters:
    -----------
    df : DataFrame
        å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
    percentile : float
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆ20-60ï¼‰
    time_diff_seconds : float
        ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”

    Returns:
    --------
    dict
        è©•ä¾¡çµæœ
    """
    try:
        print(f"\nğŸ” {percentile:.1f}%ileãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆä¸­...")

        # å·®åˆ†äºˆæ¸¬ç”¨ç›®çš„å¤‰æ•°ã®ä½œæˆ
        df_with_diff_targets = create_temperature_difference_targets(
            df, [zone], [horizon], pd.Timedelta(seconds=time_diff_seconds)
        )

        # å·®åˆ†äºˆæ¸¬å°‚ç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        df_processed, selected_features, feature_info = create_difference_prediction_pipeline(
            df=df_with_diff_targets,
            zone_nums=[zone],
            horizons_minutes=[horizon],
            time_diff_seconds=time_diff_seconds,
            smoothing_window=SMOOTHING_WINDOWS[0] if SMOOTHING_WINDOWS else 5,
            feature_selection_threshold=FEATURE_SELECTION_THRESHOLD
        )

        diff_target_col = f'temp_diff_{zone}_future_{horizon}'
        if diff_target_col not in df_processed.columns:
            return {'error': f'ç›®çš„å¤‰æ•° {diff_target_col} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        feature_cols = [col for col in selected_features if col in df_processed.columns]
        valid_data = df_processed.dropna(subset=[diff_target_col] + feature_cols)

        if len(valid_data) < 100:
            return {'error': f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(valid_data)}è¡Œ)'}

        # é«˜å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        abs_diff_col = f'abs_temp_diff_{zone}_future_{horizon}'
        valid_data[abs_diff_col] = valid_data[diff_target_col].abs()

        filtered_data, filter_info = filter_high_value_targets(
            valid_data, [abs_diff_col], percentile=percentile
        )

        if len(filtered_data) < 50:  # ã‚ˆã‚Šå³ã—ã„æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°è¦ä»¶
            return {'error': f'ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(filtered_data)}è¡Œ)'}

        # æ™‚ç³»åˆ—åˆ†å‰²
        train_df, test_df = get_time_based_train_test_split(filtered_data, test_size=TEST_SIZE)

        X_train = train_df[feature_cols]
        y_train_diff = train_df[diff_target_col]
        X_test = test_df[feature_cols]
        y_test_diff = test_df[diff_target_col]

        if len(X_train) < 30 or len(X_test) < 15:  # ã‚ˆã‚Šå³ã—ã„æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°è¦ä»¶
            return {'error': f'åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ (train: {len(X_train)}, test: {len(X_test)})'}

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        start_time = time.time()
        diff_model = train_temperature_difference_model(X_train, y_train_diff)
        training_time = time.time() - start_time

        # äºˆæ¸¬
        y_pred_diff = diff_model.predict(X_test)

        # è©•ä¾¡
        current_temps_test = test_df[f'sens_temp_{zone}']
        diff_metrics = evaluate_temperature_difference_model(
            y_test_diff, y_pred_diff, current_temps_test
        )

        # è¿½åŠ ã®çµ±è¨ˆæƒ…å ±
        diff_std = y_test_diff.std()
        diff_mean_abs = y_test_diff.abs().mean()
        pred_std = pd.Series(y_pred_diff).std()

        # ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™
        large_changes = (y_test_diff.abs() > diff_mean_abs * 2).sum()
        small_changes = (y_test_diff.abs() < diff_mean_abs * 0.5).sum()

        # çµæœã®æ•´ç†
        result = {
            'percentile': percentile,
            'original_data_count': len(valid_data),
            'filtered_data_count': len(filtered_data),
            'data_reduction_rate': (len(valid_data) - len(filtered_data)) / len(valid_data) * 100,
            'train_count': len(X_train),
            'test_count': len(X_test),
            'training_time': training_time,
            'threshold_value': filter_info['thresholds'][abs_diff_col],
            'diff_rmse': diff_metrics['diff_rmse'],
            'diff_mae': diff_metrics['diff_mae'],
            'direction_accuracy': diff_metrics['direction_accuracy'],
            'large_change_accuracy': diff_metrics.get('large_change_accuracy', np.nan),
            'restoration_rmse': diff_metrics.get('restoration_rmse', np.nan),
            'restoration_mae': diff_metrics.get('restoration_mae', np.nan),
            'restoration_r2': diff_metrics.get('restoration_r2', np.nan),
            # è¿½åŠ çµ±è¨ˆ
            'diff_std': diff_std,
            'diff_mean_abs': diff_mean_abs,
            'pred_std': pred_std,
            'large_changes_count': large_changes,
            'small_changes_count': small_changes,
            'data_diversity_score': large_changes + small_changes,  # ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§æŒ‡æ¨™
            'error': None
        }

        print(f"âœ… {percentile:.1f}%ileå®Œäº†: RMSE={result['restoration_rmse']:.4f}, RÂ²={result['restoration_r2']:.4f}, ãƒ‡ãƒ¼ã‚¿æ•°={len(filtered_data)}")
        return result

    except Exception as e:
        print(f"âŒ {percentile:.1f}%ileã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'percentile': percentile,
            'error': str(e),
            'diff_rmse': np.nan,
            'diff_mae': np.nan,
            'restoration_rmse': np.nan,
            'restoration_mae': np.nan,
            'restoration_r2': np.nan
        }


def optimize_filtering_fine_tuned(zone=1, horizon=15):
    """
    ç´°ã‹ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°

    è¤‡æ•°ã®ç¯„å›²ã§æ®µéšçš„ã«æœ€é©åŒ–ï¼š
    1. 20-60%ã‚’5%åˆ»ã¿ï¼ˆç²—ã„æ¢ç´¢ï¼‰
    2. æœ€è‰¯çµæœå‘¨è¾ºã‚’2%åˆ»ã¿ï¼ˆä¸­ç¨‹åº¦æ¢ç´¢ï¼‰
    3. ã•ã‚‰ã«æœ€è‰¯çµæœå‘¨è¾ºã‚’0.5%åˆ»ã¿ï¼ˆç´°ã‹ã„æ¢ç´¢ï¼‰

    Parameters:
    -----------
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³

    Returns:
    --------
    dict
        æœ€é©åŒ–çµæœ
    """
    print("ğŸ”¥ ç´°ã‹ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤æœ€é©åŒ–é–‹å§‹")
    print(f"å¯¾è±¡: ã‚¾ãƒ¼ãƒ³{zone}, {horizon}åˆ†å¾Œäºˆæ¸¬")
    print("æ®µéšçš„æœ€é©åŒ–: ç²—ã„æ¢ç´¢ â†’ ä¸­ç¨‹åº¦æ¢ç´¢ â†’ ç´°ã‹ã„æ¢ç´¢")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    try:
        df = pd.read_csv('AllDayData.csv')
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    # åŸºæœ¬å‰å‡¦ç†
    print("\n## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†...")
    df = prepare_time_features(df)
    df = filter_temperature_outliers(df)

    # æ™‚é–“å·®ã®è¨ˆç®—
    time_diff_seconds = df.index.to_series().diff().dropna().value_counts().index[0].total_seconds()

    all_results = []

    # ã‚¹ãƒ†ãƒƒãƒ—1: ç²—ã„æ¢ç´¢ï¼ˆ20-60%ã‚’5%åˆ»ã¿ï¼‰
    print("\n" + "="*80)
    print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: ç²—ã„æ¢ç´¢ï¼ˆ20-60%ã‚’5%åˆ»ã¿ï¼‰")
    print("="*80)

    coarse_percentiles = np.arange(20, 65, 5)  # [20, 25, 30, 35, 40, 45, 50, 55, 60]
    coarse_results = []

    for i, percentile in enumerate(coarse_percentiles, 1):
        print(f"\n[{i}/{len(coarse_percentiles)}] {percentile}%ileãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = test_filtering_threshold_fine(df, zone, horizon, percentile, time_diff_seconds)
        coarse_results.append(result)
        all_results.append(result)

    # ç²—ã„æ¢ç´¢ã®æœ€è‰¯çµæœã‚’ç‰¹å®š
    valid_coarse = [r for r in coarse_results if r['error'] is None and not np.isnan(r['restoration_rmse'])]
    if not valid_coarse:
        print("âŒ ç²—ã„æ¢ç´¢ã§æœ‰åŠ¹ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    best_coarse = min(valid_coarse, key=lambda x: x['restoration_rmse'])
    print(f"\nğŸ† ç²—ã„æ¢ç´¢æœ€è‰¯çµæœ: {best_coarse['percentile']}%ile (RMSE: {best_coarse['restoration_rmse']:.4f})")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ä¸­ç¨‹åº¦æ¢ç´¢ï¼ˆæœ€è‰¯çµæœÂ±10%ã‚’2%åˆ»ã¿ï¼‰
    print("\n" + "="*80)
    print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ä¸­ç¨‹åº¦æ¢ç´¢ï¼ˆæœ€è‰¯çµæœå‘¨è¾ºã‚’2%åˆ»ã¿ï¼‰")
    print("="*80)

    center = best_coarse['percentile']
    medium_start = max(20, center - 10)
    medium_end = min(60, center + 10)
    medium_percentiles = np.arange(medium_start, medium_end + 1, 2)

    # æ—¢ã«ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®å€¤ã‚’é™¤å¤–
    tested_percentiles = {r['percentile'] for r in coarse_results}
    medium_percentiles = [p for p in medium_percentiles if p not in tested_percentiles]

    print(f"ä¸­ç¨‹åº¦æ¢ç´¢ç¯„å›²: {medium_start}%-{medium_end}% (ä¸­å¿ƒ: {center}%)")

    medium_results = []
    for i, percentile in enumerate(medium_percentiles, 1):
        print(f"\n[{i}/{len(medium_percentiles)}] {percentile}%ileãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = test_filtering_threshold_fine(df, zone, horizon, percentile, time_diff_seconds)
        medium_results.append(result)
        all_results.append(result)

    # ä¸­ç¨‹åº¦æ¢ç´¢ã‚’å«ã‚ãŸæœ€è‰¯çµæœã‚’ç‰¹å®š
    all_valid = [r for r in all_results if r['error'] is None and not np.isnan(r['restoration_rmse'])]
    best_medium = min(all_valid, key=lambda x: x['restoration_rmse'])
    print(f"\nğŸ† ä¸­ç¨‹åº¦æ¢ç´¢æœ€è‰¯çµæœ: {best_medium['percentile']}%ile (RMSE: {best_medium['restoration_rmse']:.4f})")

    # ã‚¹ãƒ†ãƒƒãƒ—3: ç´°ã‹ã„æ¢ç´¢ï¼ˆæœ€è‰¯çµæœÂ±5%ã‚’0.5%åˆ»ã¿ï¼‰
    print("\n" + "="*80)
    print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: ç´°ã‹ã„æ¢ç´¢ï¼ˆæœ€è‰¯çµæœå‘¨è¾ºã‚’0.5%åˆ»ã¿ï¼‰")
    print("="*80)

    center = best_medium['percentile']
    fine_start = max(20, center - 5)
    fine_end = min(60, center + 5)
    fine_percentiles = np.arange(fine_start, fine_end + 0.5, 0.5)

    # æ—¢ã«ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®å€¤ã‚’é™¤å¤–
    tested_percentiles = {r['percentile'] for r in all_results}
    fine_percentiles = [p for p in fine_percentiles if p not in tested_percentiles]

    print(f"ç´°ã‹ã„æ¢ç´¢ç¯„å›²: {fine_start}%-{fine_end}% (ä¸­å¿ƒ: {center}%)")

    fine_results = []
    for i, percentile in enumerate(fine_percentiles, 1):
        print(f"\n[{i}/{len(fine_percentiles)}] {percentile:.1f}%ileãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = test_filtering_threshold_fine(df, zone, horizon, percentile, time_diff_seconds)
        fine_results.append(result)
        all_results.append(result)

    # æœ€çµ‚çµæœåˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š æœ€çµ‚çµæœåˆ†æ")
    print("="*80)

    # æœ‰åŠ¹ãªçµæœã®ã¿ã‚’æŠ½å‡º
    valid_results = [r for r in all_results if r['error'] is None and not np.isnan(r['restoration_rmse'])]

    if not valid_results:
        print("âŒ æœ‰åŠ¹ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    # å„æŒ‡æ¨™ã§ã®æœ€è‰¯çµæœ
    best_rmse = min(valid_results, key=lambda x: x['restoration_rmse'])
    best_mae = min(valid_results, key=lambda x: x['restoration_mae'])
    best_r2 = max(valid_results, key=lambda x: x['restoration_r2'])
    best_direction = max(valid_results, key=lambda x: x['direction_accuracy'])

    print(f"\nğŸ† æœ€è‰¯çµæœ:")
    print(f"æœ€ä½RMSE: {best_rmse['restoration_rmse']:.4f} ({best_rmse['percentile']:.1f}%ile)")
    print(f"æœ€ä½MAE: {best_mae['restoration_mae']:.4f} ({best_mae['percentile']:.1f}%ile)")
    print(f"æœ€é«˜RÂ²: {best_r2['restoration_r2']:.4f} ({best_r2['percentile']:.1f}%ile)")
    print(f"æœ€é«˜æ–¹å‘ç²¾åº¦: {best_direction['direction_accuracy']:.1f}% ({best_direction['percentile']:.1f}%ile)")

    # æ”¹è‰¯ã•ã‚ŒãŸç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    for result in valid_results:
        # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        rmse_range = max(r['restoration_rmse'] for r in valid_results) - min(r['restoration_rmse'] for r in valid_results)
        mae_range = max(r['restoration_mae'] for r in valid_results) - min(r['restoration_mae'] for r in valid_results)
        r2_range = max(r['restoration_r2'] for r in valid_results) - min(r['restoration_r2'] for r in valid_results)

        if rmse_range > 0:
            rmse_score = 1 - (result['restoration_rmse'] - min(r['restoration_rmse'] for r in valid_results)) / rmse_range
        else:
            rmse_score = 1.0

        if mae_range > 0:
            mae_score = 1 - (result['restoration_mae'] - min(r['restoration_mae'] for r in valid_results)) / mae_range
        else:
            mae_score = 1.0

        if r2_range > 0:
            r2_score = (result['restoration_r2'] - min(r['restoration_r2'] for r in valid_results)) / r2_range
        else:
            r2_score = 1.0

        # ãƒ‡ãƒ¼ã‚¿é‡ãƒœãƒ¼ãƒŠã‚¹ï¼ˆååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        data_bonus = min(1.0, result['filtered_data_count'] / 50000)  # 50,000è¡Œã‚’åŸºæº–

        # é‡ã¿ä»˜ã‘ç·åˆã‚¹ã‚³ã‚¢ï¼ˆRMSEé‡è¦–ã€ãƒ‡ãƒ¼ã‚¿é‡ã‚‚è€ƒæ…®ï¼‰
        result['composite_score'] = (0.4 * rmse_score + 0.25 * mae_score + 0.25 * r2_score + 0.1 * data_bonus)

    # ç·åˆæœ€è‰¯çµæœ
    best_overall = max(valid_results, key=lambda x: x['composite_score'])

    print(f"\nğŸ¯ ç·åˆæœ€é©é–¾å€¤: {best_overall['percentile']:.1f}%ile")
    print(f"  å¾©å…ƒRMSE: {best_overall['restoration_rmse']:.4f}")
    print(f"  å¾©å…ƒMAE: {best_overall['restoration_mae']:.4f}")
    print(f"  å¾©å…ƒRÂ²: {best_overall['restoration_r2']:.4f}")
    print(f"  æ–¹å‘ç²¾åº¦: {best_overall['direction_accuracy']:.1f}%")
    print(f"  ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›: {best_overall['data_reduction_rate']:.1f}%")
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿æ•°: {best_overall['filtered_data_count']:,}è¡Œ")
    print(f"  ç·åˆã‚¹ã‚³ã‚¢: {best_overall['composite_score']:.4f}")

    # è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆä¸Šä½10ä½ï¼‰
    print(f"\nğŸ“‹ è©³ç´°çµæœï¼ˆä¸Šä½10ä½ï¼‰:")
    print("Percentile | RMSE   | MAE    | RÂ²     | Dir%  | Dataæ•°  | Score")
    print("-" * 70)
    top_results = sorted(valid_results, key=lambda x: x['composite_score'], reverse=True)[:10]
    for result in top_results:
        print(f"{result['percentile']:9.1f}% | {result['restoration_rmse']:6.4f} | "
              f"{result['restoration_mae']:6.4f} | {result['restoration_r2']:6.4f} | "
              f"{result['direction_accuracy']:5.1f} | {result['filtered_data_count']:7,} | "
              f"{result['composite_score']:5.3f}")

    # çµæœä¿å­˜
    output_dir = Path("Output")
    output_dir.mkdir(exist_ok=True)

    optimization_result = {
        'zone': zone,
        'horizon': horizon,
        'optimization_type': 'fine_tuned',
        'total_tests': len(all_results),
        'valid_tests': len(valid_results),
        'best_overall': best_overall,
        'best_rmse': best_rmse,
        'best_mae': best_mae,
        'best_r2': best_r2,
        'best_direction': best_direction,
        'top_10_results': top_results,
        'all_results': all_results,
        'valid_results': valid_results,
        'coarse_results': coarse_results,
        'medium_results': medium_results,
        'fine_results': fine_results
    }

    result_file = output_dir / f"filtering_optimization_fine_tuned_zone_{zone}_horizon_{horizon}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        # NaNå€¤ã‚’Nullã«å¤‰æ›ã—ã¦JSONä¿å­˜
        json.dump(optimization_result, f, indent=2, default=lambda x: None if pd.isna(x) else x, ensure_ascii=False)

    print(f"\nğŸ’¾ çµæœä¿å­˜: {result_file}")

    return optimization_result


if __name__ == "__main__":
    # ç´°ã‹ã„æœ€é©åŒ–å®Ÿè¡Œ
    result = optimize_filtering_fine_tuned(
        zone=1,
        horizon=15
    )

    if result:
        print(f"\nğŸ‰ ç´°ã‹ã„æœ€é©åŒ–å®Œäº†ï¼")
        print(f"æ¨å¥¨é–¾å€¤: {result['best_overall']['percentile']:.1f}%ile")
        print(f"RMSEæ”¹å–„: {result['best_overall']['restoration_rmse']:.4f}")
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {result['total_tests']}å›")
    else:
        print("\nâŒ ç´°ã‹ã„æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
