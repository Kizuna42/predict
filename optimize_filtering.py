#!/usr/bin/env python
# coding: utf-8

"""
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç•°ãªã‚‹ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€æœ€é©ãªé–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import time

warnings.filterwarnings('ignore')

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import SMOOTHING_WINDOWS, FEATURE_SELECTION_THRESHOLD, TEST_SIZE

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.preprocessing import (
    filter_temperature_outliers,
    create_temperature_difference_targets,
    prepare_time_features,
    get_time_based_train_test_split,
    filter_high_value_targets
)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.data.feature_engineering import create_difference_prediction_pipeline

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.training import train_temperature_difference_model

# è©•ä¾¡é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.models.evaluation import evaluate_temperature_difference_model


def test_filtering_threshold(df, zone, horizon, percentile, time_diff_seconds):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€æ€§èƒ½ã‚’è©•ä¾¡

    Parameters:
    -----------
    df : DataFrame
        å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
    percentile : float
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆ50-99ï¼‰
    time_diff_seconds : float
        ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”

    Returns:
    --------
    dict
        è©•ä¾¡çµæœ
    """
    try:
        print(f"\nğŸ” {percentile}%ileãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆä¸­...")

        # å·®åˆ†äºˆæ¸¬ç”¨ç›®çš„å¤‰æ•°ã®ä½œæˆ
        df_with_diff_targets = create_temperature_difference_targets(
            df, [zone], [horizon], pd.Timedelta(seconds=time_diff_seconds)
        )

        # å·®åˆ†äºˆæ¸¬å°‚ç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        df_processed, selected_features, feature_info = create_difference_prediction_pipeline(
            df=df_with_diff_targets,
            zone_nums=[zone],
            horizons_minutes=[horizon],
            time_diff_seconds=time_diff_seconds,
            smoothing_window=SMOOTHING_WINDOWS[0] if SMOOTHING_WINDOWS else 5,
            feature_selection_threshold=FEATURE_SELECTION_THRESHOLD
        )

        diff_target_col = f'temp_diff_{zone}_future_{horizon}'
        if diff_target_col not in df_processed.columns:
            return {'error': f'ç›®çš„å¤‰æ•° {diff_target_col} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        feature_cols = [col for col in selected_features if col in df_processed.columns]
        valid_data = df_processed.dropna(subset=[diff_target_col] + feature_cols)

        if len(valid_data) < 100:
            return {'error': f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(valid_data)}è¡Œ)'}

        # é«˜å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        abs_diff_col = f'abs_temp_diff_{zone}_future_{horizon}'
        valid_data[abs_diff_col] = valid_data[diff_target_col].abs()

        filtered_data, filter_info = filter_high_value_targets(
            valid_data, [abs_diff_col], percentile=percentile
        )

        if len(filtered_data) < 30:
            return {'error': f'ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(filtered_data)}è¡Œ)'}

        # æ™‚ç³»åˆ—åˆ†å‰²
        train_df, test_df = get_time_based_train_test_split(filtered_data, test_size=TEST_SIZE)

        X_train = train_df[feature_cols]
        y_train_diff = train_df[diff_target_col]
        X_test = test_df[feature_cols]
        y_test_diff = test_df[diff_target_col]

        if len(X_train) < 20 or len(X_test) < 10:
            return {'error': f'åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ (train: {len(X_train)}, test: {len(X_test)})'}

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        start_time = time.time()
        diff_model = train_temperature_difference_model(X_train, y_train_diff)
        training_time = time.time() - start_time

        # äºˆæ¸¬
        y_pred_diff = diff_model.predict(X_test)

        # è©•ä¾¡
        current_temps_test = test_df[f'sens_temp_{zone}']
        diff_metrics = evaluate_temperature_difference_model(
            y_test_diff, y_pred_diff, current_temps_test
        )

        # çµæœã®æ•´ç†
        result = {
            'percentile': percentile,
            'original_data_count': len(valid_data),
            'filtered_data_count': len(filtered_data),
            'data_reduction_rate': (len(valid_data) - len(filtered_data)) / len(valid_data) * 100,
            'train_count': len(X_train),
            'test_count': len(X_test),
            'training_time': training_time,
            'threshold_value': filter_info['thresholds'][abs_diff_col],
            'diff_rmse': diff_metrics['diff_rmse'],
            'diff_mae': diff_metrics['diff_mae'],
            'direction_accuracy': diff_metrics['direction_accuracy'],
            'large_change_accuracy': diff_metrics.get('large_change_accuracy', np.nan),
            'restoration_rmse': diff_metrics.get('restoration_rmse', np.nan),
            'restoration_mae': diff_metrics.get('restoration_mae', np.nan),
            'restoration_r2': diff_metrics.get('restoration_r2', np.nan),
            'error': None
        }

        print(f"âœ… {percentile}%ileå®Œäº†: RMSE={result['restoration_rmse']:.4f}, RÂ²={result['restoration_r2']:.4f}")
        return result

    except Exception as e:
        print(f"âŒ {percentile}%ileã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'percentile': percentile,
            'error': str(e),
            'diff_rmse': np.nan,
            'diff_mae': np.nan,
            'restoration_rmse': np.nan,
            'restoration_mae': np.nan,
            'restoration_r2': np.nan
        }


def optimize_filtering_threshold(zone=1, horizon=15, percentile_range=(60, 95, 5)):
    """
    ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤ã‚’æœ€é©åŒ–ã™ã‚‹é–¢æ•°

    Parameters:
    -----------
    zone : int
        å¯¾è±¡ã‚¾ãƒ¼ãƒ³
    horizon : int
        äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
    percentile_range : tuple
        (é–‹å§‹%, çµ‚äº†%, ã‚¹ãƒ†ãƒƒãƒ—%)

    Returns:
    --------
    dict
        æœ€é©åŒ–çµæœ
    """
    print("ğŸ”¥ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤æœ€é©åŒ–é–‹å§‹")
    print(f"å¯¾è±¡: ã‚¾ãƒ¼ãƒ³{zone}, {horizon}åˆ†å¾Œäºˆæ¸¬")
    print(f"ãƒ†ã‚¹ãƒˆç¯„å›²: {percentile_range[0]}%ï½{percentile_range[1]}% (ã‚¹ãƒ†ãƒƒãƒ—: {percentile_range[2]}%)")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    try:
        df = pd.read_csv('AllDayData.csv')
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    # åŸºæœ¬å‰å‡¦ç†
    print("\n## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†...")
    df = prepare_time_features(df)
    df = filter_temperature_outliers(df)

    # æ™‚é–“å·®ã®è¨ˆç®—
    time_diff_seconds = df.index.to_series().diff().dropna().value_counts().index[0].total_seconds()

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = []
    percentiles = range(percentile_range[0], percentile_range[1] + 1, percentile_range[2])

    print(f"\n## {len(percentiles)}å€‹ã®é–¾å€¤ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")

    for i, percentile in enumerate(percentiles, 1):
        print(f"\n[{i}/{len(percentiles)}] {percentile}%ileãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = test_filtering_threshold(df, zone, horizon, percentile, time_diff_seconds)
        results.append(result)

        # é€²æ—è¡¨ç¤º
        if result['error'] is None:
            print(f"  ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›: {result['data_reduction_rate']:.1f}%")
            print(f"  å¾©å…ƒRMSE: {result['restoration_rmse']:.4f}")
            print(f"  å¾©å…ƒRÂ²: {result['restoration_r2']:.4f}")
        else:
            print(f"  ã‚¨ãƒ©ãƒ¼: {result['error']}")

    # çµæœåˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š æœ€é©åŒ–çµæœåˆ†æ")
    print("="*80)

    # æœ‰åŠ¹ãªçµæœã®ã¿ã‚’æŠ½å‡º
    valid_results = [r for r in results if r['error'] is None and not np.isnan(r['restoration_rmse'])]

    if not valid_results:
        print("âŒ æœ‰åŠ¹ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    # å„æŒ‡æ¨™ã§ã®æœ€è‰¯çµæœ
    best_rmse = min(valid_results, key=lambda x: x['restoration_rmse'])
    best_mae = min(valid_results, key=lambda x: x['restoration_mae'])
    best_r2 = max(valid_results, key=lambda x: x['restoration_r2'])
    best_direction = max(valid_results, key=lambda x: x['direction_accuracy'])

    print(f"\nğŸ† æœ€è‰¯çµæœ:")
    print(f"æœ€ä½RMSE: {best_rmse['restoration_rmse']:.4f} ({best_rmse['percentile']}%ile)")
    print(f"æœ€ä½MAE: {best_mae['restoration_mae']:.4f} ({best_mae['percentile']}%ile)")
    print(f"æœ€é«˜RÂ²: {best_r2['restoration_r2']:.4f} ({best_r2['percentile']}%ile)")
    print(f"æœ€é«˜æ–¹å‘ç²¾åº¦: {best_direction['direction_accuracy']:.1f}% ({best_direction['percentile']}%ile)")

    # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆè¤‡æ•°æŒ‡æ¨™ã®é‡ã¿ä»˜ã‘å¹³å‡ï¼‰
    for result in valid_results:
        # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        rmse_score = 1 - (result['restoration_rmse'] - min(r['restoration_rmse'] for r in valid_results)) / \
                     (max(r['restoration_rmse'] for r in valid_results) - min(r['restoration_rmse'] for r in valid_results) + 1e-8)
        mae_score = 1 - (result['restoration_mae'] - min(r['restoration_mae'] for r in valid_results)) / \
                    (max(r['restoration_mae'] for r in valid_results) - min(r['restoration_mae'] for r in valid_results) + 1e-8)
        r2_score = (result['restoration_r2'] - min(r['restoration_r2'] for r in valid_results)) / \
                   (max(r['restoration_r2'] for r in valid_results) - min(r['restoration_r2'] for r in valid_results) + 1e-8)

        # é‡ã¿ä»˜ã‘ç·åˆã‚¹ã‚³ã‚¢ï¼ˆRMSEé‡è¦–ï¼‰
        result['composite_score'] = 0.4 * rmse_score + 0.3 * mae_score + 0.3 * r2_score

    # ç·åˆæœ€è‰¯çµæœ
    best_overall = max(valid_results, key=lambda x: x['composite_score'])

    print(f"\nğŸ¯ ç·åˆæœ€é©é–¾å€¤: {best_overall['percentile']}%ile")
    print(f"  å¾©å…ƒRMSE: {best_overall['restoration_rmse']:.4f}")
    print(f"  å¾©å…ƒMAE: {best_overall['restoration_mae']:.4f}")
    print(f"  å¾©å…ƒRÂ²: {best_overall['restoration_r2']:.4f}")
    print(f"  æ–¹å‘ç²¾åº¦: {best_overall['direction_accuracy']:.1f}%")
    print(f"  ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›: {best_overall['data_reduction_rate']:.1f}%")
    print(f"  ç·åˆã‚¹ã‚³ã‚¢: {best_overall['composite_score']:.4f}")

    # è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«
    print(f"\nğŸ“‹ è©³ç´°çµæœ:")
    print("Percentile | RMSE   | MAE    | RÂ²     | Dir%  | Data% | Score")
    print("-" * 65)
    for result in sorted(valid_results, key=lambda x: x['percentile']):
        print(f"{result['percentile']:9.0f}% | {result['restoration_rmse']:6.4f} | "
              f"{result['restoration_mae']:6.4f} | {result['restoration_r2']:6.4f} | "
              f"{result['direction_accuracy']:5.1f} | {result['data_reduction_rate']:5.1f} | "
              f"{result['composite_score']:5.3f}")

    # çµæœä¿å­˜
    output_dir = Path("Output")
    output_dir.mkdir(exist_ok=True)

    optimization_result = {
        'zone': zone,
        'horizon': horizon,
        'test_range': percentile_range,
        'best_overall': best_overall,
        'best_rmse': best_rmse,
        'best_mae': best_mae,
        'best_r2': best_r2,
        'best_direction': best_direction,
        'all_results': results,
        'valid_results': valid_results
    }

    result_file = output_dir / f"filtering_optimization_zone_{zone}_horizon_{horizon}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        # NaNå€¤ã‚’Nullã«å¤‰æ›ã—ã¦JSONä¿å­˜
        json.dump(optimization_result, f, indent=2, default=lambda x: None if pd.isna(x) else x)

    print(f"\nğŸ’¾ çµæœä¿å­˜: {result_file}")

    return optimization_result


if __name__ == "__main__":
    # æœ€é©åŒ–å®Ÿè¡Œ
    result = optimize_filtering_threshold(
        zone=1,
        horizon=15,
        percentile_range=(60, 95, 5)  # 60%ã‹ã‚‰95%ã¾ã§5%åˆ»ã¿
    )

    if result:
        print(f"\nğŸ‰ æœ€é©åŒ–å®Œäº†ï¼æ¨å¥¨é–¾å€¤: {result['best_overall']['percentile']}%ile")
    else:
        print("\nâŒ æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
