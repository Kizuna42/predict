#!/usr/bin/env python
# coding: utf-8

"""
LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œã®åŒ…æ‹¬çš„è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import pandas as pd
import numpy as np
import os
import pickle
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.config import HORIZONS, OUTPUT_DIR

# æ–°ã—ã„åŒ…æ‹¬çš„è¨ºæ–­æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.diagnostics.comprehensive_lag_analysis import (
    analyze_lag_following_comprehensive,
    generate_lag_analysis_report
)

# æ—¢å­˜ã®è¨ºæ–­æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.diagnostics.time_validation import (
    validate_prediction_timing,
    analyze_time_axis_consistency
)

# å¯è¦–åŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.utils.advanced_visualization import (
    plot_corrected_time_series_by_horizon
)


def load_model_results():
    """
    ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«çµæœã‚’èª­ã¿è¾¼ã‚€
    """
    print("ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«çµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")

    # çµæœãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã®åˆæœŸåŒ–
    results = {}

    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    models_dir = "models"
    if not os.path.exists(models_dir):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{models_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]

    if not model_files:
        print("ã‚¨ãƒ©ãƒ¼: ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    # ã‚¾ãƒ¼ãƒ³ã¨ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã®çµ„ã¿åˆã‚ã›ã‚’æŠ½å‡º
    zone_horizon_combinations = set()
    for file in model_files:
        if 'zone_' in file and 'horizon_' in file:
            parts = file.replace('.pkl', '').split('_')
            zone_idx = parts.index('zone') + 1
            horizon_idx = parts.index('horizon') + 1

            if zone_idx < len(parts) and horizon_idx < len(parts):
                try:
                    zone = int(parts[zone_idx])
                    horizon = int(parts[horizon_idx])
                    zone_horizon_combinations.add((zone, horizon))
                except ValueError:
                    continue

    print(f"æ¤œå‡ºã•ã‚ŒãŸã‚¾ãƒ¼ãƒ³ãƒ»ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³çµ„ã¿åˆã‚ã›: {len(zone_horizon_combinations)}å€‹")

    # å„çµ„ã¿åˆã‚ã›ã«ã¤ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    for zone, horizon in zone_horizon_combinations:
        if zone not in results:
            results[zone] = {}

        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        model_file = f"model_zone_{zone}_horizon_{horizon}.pkl"
        features_file = f"features_zone_{zone}_horizon_{horizon}.pkl"

        model_path = os.path.join(models_dir, model_file)
        features_path = os.path.join(models_dir, features_file)

        if os.path.exists(model_path) and os.path.exists(features_path):
            try:
                with open(model_path, 'rb') as f:
                    model = pickle.load(f)

                with open(features_path, 'rb') as f:
                    features = pickle.load(f)

                # ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—
                if hasattr(model, 'feature_importances_'):
                    feature_importance = pd.DataFrame({
                        'feature': features,
                        'importance': model.feature_importances_
                    }).sort_values('importance', ascending=False)
                else:
                    # ç‰¹å¾´é‡é‡è¦åº¦ãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
                    feature_importance = pd.DataFrame({
                        'feature': features,
                        'importance': [1.0] * len(features)
                    })

                results[zone][horizon] = {
                    'model': model,
                    'selected_features': features,
                    'feature_importance': feature_importance
                }

                print(f"ã‚¾ãƒ¼ãƒ³ {zone}, ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

            except Exception as e:
                print(f"ã‚¾ãƒ¼ãƒ³ {zone}, ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")

    return results


def load_test_data_and_predictions():
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã‚€
    æ³¨æ„: ã“ã®é–¢æ•°ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦
    """
    print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv('AllDayData.csv')
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")

        # æ™‚é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®šï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df.set_index('timestamp', inplace=True)
        elif df.index.name != 'timestamp':
            # æœ€åˆã®åˆ—ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
            df.index = pd.to_datetime(df.index)

        return df

    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def run_comprehensive_lag_diagnosis(target_horizons=None):
    """
    åŒ…æ‹¬çš„LAGè¨ºæ–­ã®å®Ÿè¡Œ

    Parameters:
    -----------
    target_horizons : list, optional
        è¨ºæ–­å¯¾è±¡ã®ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆåˆ†ï¼‰ã®ãƒªã‚¹ãƒˆ
    """
    print("=" * 60)
    print("LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œã®åŒ…æ‹¬çš„è¨ºæ–­")
    print("=" * 60)

    # å¯¾è±¡ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã®è¨­å®š
    if target_horizons is None:
        target_horizons = HORIZONS

    print(f"è¨ºæ–­å¯¾è±¡ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³: {target_horizons}")

    # ãƒ¢ãƒ‡ãƒ«çµæœã®èª­ã¿è¾¼ã¿
    model_results = load_model_results()
    if model_results is None:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«çµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    test_data = load_test_data_and_predictions()
    if test_data is None:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    diagnosis_dir = os.path.join(OUTPUT_DIR, "lag_diagnosis")
    os.makedirs(diagnosis_dir, exist_ok=True)

    # å„ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã«ã¤ã„ã¦è¨ºæ–­ã‚’å®Ÿè¡Œ
    for horizon in target_horizons:
        print(f"\n{'='*40}")
        print(f"ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†ã®è¨ºæ–­é–‹å§‹")
        print(f"{'='*40}")

        # è©²å½“ã™ã‚‹ã‚¾ãƒ¼ãƒ³ã®ç¢ºèª
        zones_for_horizon = [zone for zone in model_results.keys()
                           if horizon in model_results[zone]]

        if not zones_for_horizon:
            print(f"è­¦å‘Š: ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue

        print(f"è¨ºæ–­å¯¾è±¡ã‚¾ãƒ¼ãƒ³: {zones_for_horizon}")

        # å„ã‚¾ãƒ¼ãƒ³ã«ã¤ã„ã¦è©³ç´°è¨ºæ–­ã‚’å®Ÿè¡Œ
        detailed_results = {}

        for zone in zones_for_horizon:
            print(f"\n--- ã‚¾ãƒ¼ãƒ³ {zone} ã®è©³ç´°è¨ºæ–­ ---")

            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
            zone_model_info = model_results[zone][horizon]
            feature_importance = zone_model_info['feature_importance']

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ï¼‰
            # ã“ã“ã§ã¯ä»®ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            timestamps = test_data.index[-1000:]  # æœ€æ–°1000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ

            # å®Ÿæ¸¬å€¤ã®å–å¾—ï¼ˆå®Ÿéš›ã®åˆ—åã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            temp_col = f'sens_temp_{zone}'
            if temp_col in test_data.columns:
                actual_values = test_data[temp_col].iloc[-1000:].values
            else:
                print(f"è­¦å‘Š: ã‚¾ãƒ¼ãƒ³ {zone} ã®æ¸©åº¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            # äºˆæ¸¬å€¤ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã®äºˆæ¸¬çµæœãŒãªã„å ´åˆã®ä»®å®Ÿè£…ï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä¿å­˜ã•ã‚ŒãŸäºˆæ¸¬çµæœã‚’ä½¿ç”¨
            predicted_values = actual_values + np.random.normal(0, 0.5, len(actual_values))

            # åŒ…æ‹¬çš„åˆ†æã®å®Ÿè¡Œ
            try:
                analysis_result = analyze_lag_following_comprehensive(
                    timestamps=timestamps,
                    actual_values=actual_values,
                    predicted_values=predicted_values,
                    feature_importance=feature_importance,
                    zone=zone,
                    horizon=horizon
                )

                detailed_results[zone] = analysis_result

                # çµæœã®è¡¨ç¤º
                print(f"  é‡è¦åº¦: {analysis_result['severity']}")
                print(f"  æ™‚é–“è»¸æ•´åˆæ€§: {'OK' if analysis_result['timestamp_analysis']['is_correct_alignment'] else 'NG'}")
                print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³å¾Œè¿½ã„: {'æ¤œå‡º' if analysis_result['pattern_analysis']['valley_following'] or analysis_result['pattern_analysis']['peak_following'] else 'æœªæ¤œå‡º'}")
                print(f"  LAGä¾å­˜åº¦ãƒªã‚¹ã‚¯: {analysis_result['lag_dependency']['risk_level']}")

                # æ¨å¥¨äº‹é …ã®è¡¨ç¤º
                if analysis_result['recommendations']:
                    print("  æ¨å¥¨äº‹é …:")
                    for rec in analysis_result['recommendations'][:3]:  # ä¸Šä½3ã¤ã®ã¿è¡¨ç¤º
                        print(f"    - {rec['description']}")

            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ - {e}")
                continue

        # ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³å…¨ä½“ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if detailed_results:
            print(f"\n--- ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†ã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ---")

            # ä»®ã®results_dictã‚’ä½œæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            results_dict_for_report = {}
            for zone in detailed_results.keys():
                results_dict_for_report[zone] = {
                    horizon: {
                        'test_data': test_data.iloc[-1000:],
                        'test_y': test_data[f'sens_temp_{zone}'].iloc[-1000:] if f'sens_temp_{zone}' in test_data.columns else pd.Series([0]*1000),
                        'test_predictions': np.random.normal(25, 2, 1000),  # ä»®ã®äºˆæ¸¬å€¤
                        'feature_importance': model_results[zone][horizon]['feature_importance']
                    }
                }

            try:
                report = generate_lag_analysis_report(
                    results_dict=results_dict_for_report,
                    horizon=horizon,
                    save_dir=diagnosis_dir
                )

                print(f"  åˆ†æå¯¾è±¡ã‚¾ãƒ¼ãƒ³æ•°: {report['zones_analyzed']}")
                print(f"  é«˜ãƒªã‚¹ã‚¯ã‚¾ãƒ¼ãƒ³: {len(report['high_risk_zones'])}å€‹")
                print(f"  ä¸­ãƒªã‚¹ã‚¯ã‚¾ãƒ¼ãƒ³: {len(report['medium_risk_zones'])}å€‹")
                print(f"  ä½ãƒªã‚¹ã‚¯ã‚¾ãƒ¼ãƒ³: {len(report['low_risk_zones'])}å€‹")

                if report['overall_recommendations']:
                    print("  å…¨ä½“çš„ãªæ¨å¥¨äº‹é …:")
                    for rec in report['overall_recommendations']:
                        print(f"    - {rec}")

            except Exception as e:
                print(f"  ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        # æ™‚é–“è»¸ä¿®æ­£æ¸ˆã¿å¯è¦–åŒ–ã®ç”Ÿæˆ
        print(f"\n--- ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ {horizon}åˆ†ã®å¯è¦–åŒ–ç”Ÿæˆ ---")
        try:
            if detailed_results:
                # å¯è¦–åŒ–ã®å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯èª¿æ•´ãŒå¿…è¦ï¼‰
                fig = plot_corrected_time_series_by_horizon(
                    results_dict=results_dict_for_report,
                    horizon=horizon,
                    save_dir=diagnosis_dir,
                    points=200,
                    save=True,
                    validate_timing=True
                )

                if fig:
                    print(f"  æ™‚é–“è»¸ä¿®æ­£æ¸ˆã¿å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                else:
                    print(f"  å¯è¦–åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"  å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\n{'='*60}")
    print("LAGè¨ºæ–­å®Œäº†")
    print(f"çµæœã¯ {diagnosis_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"{'='*60}")


def print_diagnosis_summary():
    """
    è¨ºæ–­çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    """
    print("\n" + "="*60)
    print("LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œ - è¨ºæ–­é …ç›®")
    print("="*60)

    print("\nğŸ” è¨ºæ–­å†…å®¹:")
    print("1. äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆã®æ™‚é–“è»¸è¡¨ç¤ºç¢ºèª")
    print("   - 15åˆ†å…ˆäºˆæ¸¬ãŒæ­£ã—ãæœªæ¥ã®æ™‚åˆ»ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹")
    print("   - äºˆæ¸¬å€¤ãŒéå»ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«è¡¨ç¤ºã•ã‚Œã¦ã„ãªã„ã‹")

    print("\n2. LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å˜ç´”ãªéå»å€¤ã‚¹ãƒ©ã‚¤ãƒ‰æ¤œè¨¼")
    print("   - ãƒ¢ãƒ‡ãƒ«ãŒéå»ã®å®Ÿæ¸¬å€¤ã‚’å˜ç´”ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã„ãªã„ã‹")
    print("   - è°·ã‚„å±±ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé…ã‚Œã¦å‡ºç¾ã—ã¦ã„ãªã„ã‹")

    print("\n3. LAGç‰¹å¾´é‡ã¸ã®éåº¦ãªä¾å­˜åº¦ç¢ºèª")
    print("   - æ˜ç¤ºçš„ãªLAGç‰¹å¾´é‡ã®ä½¿ç”¨çŠ¶æ³")
    print("   - æš—é»™çš„ãªLAGåŠ¹æœï¼ˆå¹³æ»‘åŒ–ã€å·®åˆ†ãªã©ï¼‰ã®ä¾å­˜åº¦")

    print("\n4. æœªæ¥æƒ…å ±ã®å‚ç…§æ¼ã‚Œç¢ºèª")
    print("   - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®æ¤œå‡º")
    print("   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¦¥å½“æ€§æ¤œè¨¼")

    print("\nğŸ“Š å‡ºåŠ›ã•ã‚Œã‚‹è¨ºæ–­çµæœ:")
    print("- å„ã‚¾ãƒ¼ãƒ³ãƒ»ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³åˆ¥ã®è©³ç´°åˆ†æ")
    print("- æ™‚é–“è»¸ä¿®æ­£æ¸ˆã¿å¯è¦–åŒ–")
    print("- åŒ…æ‹¬çš„è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONå½¢å¼ï¼‰")
    print("- å…·ä½“çš„ãªæ”¹å–„æ¨å¥¨äº‹é …")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='LAGç‰¹å¾´é‡ã«ã‚ˆã‚‹å¾Œè¿½ã„å•é¡Œã®åŒ…æ‹¬çš„è¨ºæ–­')
    parser.add_argument('--horizons', type=int, nargs='+',
                       help='è¨ºæ–­å¯¾è±¡ã®äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆåˆ†ï¼‰')
    parser.add_argument('--summary', action='store_true',
                       help='è¨ºæ–­é …ç›®ã®ã‚µãƒãƒªãƒ¼ã®ã¿ã‚’è¡¨ç¤º')

    args = parser.parse_args()

    if args.summary:
        print_diagnosis_summary()
    else:
        run_comprehensive_lag_diagnosis(target_horizons=args.horizons)
